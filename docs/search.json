[
  {
    "objectID": "posts/Perceptron-Blog-Post/index.html",
    "href": "posts/Perceptron-Blog-Post/index.html",
    "title": "Perceptron Blog",
    "section": "",
    "text": "Perceptron & Update Rule Overview\nGiven linearly separable data, the perceptron algorithm always converges as it correctly linearly separates the data in a finite number of steps. To do so, it produces a “good” vector of parameters (weights and bias) by first initializing the parameters (usually a random guess or 0), then looping through the given feature matrix and label array and updating these parameters to produce a better prediction on the current example by moving the parameters in the right direction if (and only if) the model’s prediction for said example is incorrect (does not match the example’s actual label).\nIn the fit() method, the perceptron update rule functions as follows:\n- 1*(((2*yi-1) * self.w@xi) <= 0) indicator function returns 1 if labels don't match and 0 if they do. \n- if labels don't match we add y (-1 or 1 representation of actual label) multiplied by xi (our sample vector) to our parameter vector in either the positive or negative direction, depending on our actual label's value; 0 or -1 (negative direction); 1 or +1 (positive direction)\nNOTE: In order to account for bias we modifiy our given feature matrix by adding column of 1’s and expand our weight vector to have an extra column (to store bias in).\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\nfrom perceptron import Perceptron\n\n\n\nLinearly Separable Example 1\n\nweight = 0\nhistory = [] \nmax_steps = 10000\n\np = Perceptron(weight,history)\n\nnp.random.seed(12345)\n\nn = 100 #the number of data points\np_features = 3 #the number of features ends up being 2 since we do n_features = p_features - 1\n\n\nX, y = make_blobs(n_samples = n, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n\n\n\np.fit(X,y, max_steps)\n\n\n\n\nprint(p.history[-10:])\nprint(p.w)\n\n\n\n\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n[2.10557404 3.1165449  0.25079936]\n\n\nIn this example the perceptron algorithm reaches a score of 1.0, meaning it was able to succesfully linearly separate the data.\n\nfig = plt.scatter(X[:,0], X[:,1], c = y) #means [ first_row:last_row , column_0]\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\n\nfig = draw_line(p.w, -4, 4)\n\n\n\n\nVisualization of the perceptron’s resulting linear separator\n\nfig = plt.plot(p.history) \nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\nVisualization of the number of iterations it took the perceptron algorithm to converge\n\n\nLinearly Separable Example 2\n\nweight2 = 0\nhistory2 = [] \nmax_steps = 1000000\n\np2 = Perceptron(weight,history)\n\nnp.random.seed(12345)\n\nn2 = 100 #the number of data points\np_features2 = 3 #the number of features ends up being 2 since we do n_features = p_features - 1\n\n\nX2, y2 = make_blobs(n_samples = n2, n_features = p_features2 - 1, centers = [(-1, -1.888), (10, 1.7)])\nX_2 = np.append(X2, np.ones((X2.shape[0], 1)), 1)\n\n\n\n\np2.fit(X2,y2, max_steps)\n\n\n\n#print(p2.score(X2,y2))\n\n\nprint(p2.history[-10:])\nprint(p2.w)\n\n\n[0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 1.0]\n[ 0.25738542  0.90742861 -1.74920064]\n\n\nIn this example the perceptron algorithm reaches a score of 1.0, meaning it was able to succesfully linearly separate the data.\n\nfig = plt.scatter(X2[:,0], X2[:,1], c = y2) \nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\n\nfig = draw_line(p2.w, -4, 12)\n\n\n\n\nVisualization of the perceptron’s resulting linear separator\n\nfig = plt.plot(p2.history) \nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\nVisualization of the number of iterations it took the perceptron algorithm to converge\n\n\nNon-linearly Separable Example\n\nweight3 = 0\nhistory3 = [] \nmax_steps = 10000\n\np3 = Perceptron(weight,history)\n\nnp.random.seed(12345)\n\nn3 = 100 #the number of data points\np_features3 = 4 #the number of features ends up being 2 since we do n_features = p_features - 1\n\n\nX3, y3 = make_blobs(n_samples = n3, n_features = p_features3 - 1, centers = [(-1, -1.7), (1.23, 1.7)])\nX_3 = np.append(X3, np.ones((X3.shape[0], 1)), 1)\n\n\np3.fit(X3,y3, max_steps)\n\n\nprint(p3.history[-10:])\nprint(p3.w)\n\n[0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98]\n[ 1.436715    4.80069223 -0.74920064]\n\n\nThe score did not reach 1.0, so the perceptron algorithm wasn’t able to converge signifying that the data is not linearly separable.\n\nfig = plt.scatter(X3[:,0], X3[:,1], c = y2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\n\nfig = draw_line(p3.w, -4, 4)\n\n\n\n\nVisualization of the perceptron’s resulting linear separator\n\nfig = plt.plot(p3.history) #plot is not representative of iterations (no such thing as .5 iterations) #gotta fix somehow w/ matplotlib adjusment probably\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nVisualization of the number of iterations it took the perceptron algorithm to converge\n\n\nMulti-Dimensional Example\n\nweight4 = 0\nhistory4 = [] \nmax_steps = 10000\np4 = Perceptron(weight,history)\n\nnp.random.seed(12345)\n\nn4 = 100 #the number of data points\np_features4 = 6 #the number of features ends up being 5 since we do n_features = p_features4 - 1 in make_blobs() call below\n\n\nX4, y4 = make_blobs(n_samples = n4, n_features = p_features4 - 1, centers = [(-1, -1.7,-1.5, 0, 0), (1.24, 1.7,1, 0, 0)]) #need to adjust centers to 5-dimensional space\nX_4 = np.append(X4, np.ones((X4.shape[0], 1)), 1)\n\nprint(X4.shape) #to check that our data is multi-dimensional (5-dimensions in this case) as intended, meaning there are 5 features recorded for each sample.\n\np4.fit(X4,y4, max_steps)\n\n\nprint(p4.history[-10:])\nprint(p4.w)\n\n(100, 5)\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n[4.13255343 3.42143787 1.49084004 0.05145049 0.44583831 0.3426921 ]\n\n\nIn this example the perceptron algorithm reaches a score of 1.0, meaning it was able to succesfully linearly separate the data, even in more than 2 dimensions (5 in this experiment).\n\nfig = plt.plot(p4.history) #plot is not representative of iterations (no such thing as .5 iterations) #gotta fix somehow w/ matplotlib adjusment probably\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nVisualization of the number of iterations it took the perceptron algorithm to converge\n\n\nRuntime complexity of perceptron udpate rule\nThe runtime complexity of a single iteration of the perceptron update rule is O(n) where n represents our data’s number of features. To perfom the perceptron update rule we must compute the dot product of the parameter vector and one sample vector from our modified feature matrix both of which have an entry size that’s dependent on the data’s number of features, which has multiplication and addition as its relevant operations. Our data’s number of samples doesn’t affect the time complexity of a single iteration since we only end up working with one sample (chosen randomly) at the time of the update and picking and accessing a sample in the feature matrix takes constant time."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "PerceptronNotebook.html",
    "href": "PerceptronNotebook.html",
    "title": "Perceptron",
    "section": "",
    "text": "Perceptron & Update Rule Overview\nGiven linearly separable data, the perceptron algorithm always converges as it correctly linearly separates the data in a finite number of steps. To do so, it produces a “good” vector of parameters (weights and bias) by first initializing the parameters (usually a random guess or 0), then looping through the given feature matrix and label array and updating these parameters to produce a better prediction on the current example by moving the parameters in the right direction if (and only if) the model’s prediction for said example is incorrect (does not match the example’s actual label).\nIn the fit() method, the perceptron update rule functions as follows:\n- 1*(((2*yi-1) * self.w@xi) <= 0) indicator function returns 1 if labels don't match and 0 if they do. \n- if labels don't match we add y (-1 or 1 representation of actual label) multiplied by xi (our sample vector) to our parameter vector in either the positive or negative direction, depending on our actual label's value; 0 or -1 (negative direction); 1 or +1 (positive direction)\nNOTE: In order to account for bias we modifiy our given feature matrix by adding column of 1’s and expand our weight vector to have an extra column (to store bias in).\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\nfrom perceptron import Perceptron\n\n\n\nLinearly Separable Example 1\n\nweight = 0\nhistory = [] \nmax_steps = 10000\n\np = Perceptron(weight,history)\n\nnp.random.seed(12345)\n\nn = 100 #the number of data points\np_features = 3 #the number of features ends up being 2 since we do n_features = p_features - 1\n\n\nX, y = make_blobs(n_samples = n, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n\n\n\np.fit(X,y, max_steps)\n\n\n\n\nprint(p.history[-10:])\nprint(p.w)\n\n\n\n\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n[2.10557404 3.1165449  0.25079936]\n\n\nIn this example the perceptron algorithm reaches a score of 1.0, meaning it was able to succesfully linearly separate the data.\n\nfig = plt.scatter(X[:,0], X[:,1], c = y) #means [ first_row:last_row , column_0]\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\n\nfig = draw_line(p.w, -4, 4)\n\n\n\n\nVisualization of the perceptron’s resulting linear separator\n\nfig = plt.plot(p.history) \nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\nVisualization of the number of iterations it took the perceptron algorithm to converge\n\n\nLinearly Separable Example 2\n\nweight2 = 0\nhistory2 = [] \nmax_steps = 1000000\n\np2 = Perceptron(weight,history)\n\nnp.random.seed(12345)\n\nn2 = 100 #the number of data points\np_features2 = 3 #the number of features ends up being 2 since we do n_features = p_features - 1\n\n\nX2, y2 = make_blobs(n_samples = n2, n_features = p_features2 - 1, centers = [(-1, -1.888), (10, 1.7)])\nX_2 = np.append(X2, np.ones((X2.shape[0], 1)), 1)\n\n\n\n\np2.fit(X2,y2, max_steps)\n\n\n\n#print(p2.score(X2,y2))\n\n\nprint(p2.history[-10:])\nprint(p2.w)\n\n\n[0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 1.0]\n[ 0.25738542  0.90742861 -1.74920064]\n\n\nIn this example the perceptron algorithm reaches a score of 1.0, meaning it was able to succesfully linearly separate the data.\n\nfig = plt.scatter(X2[:,0], X2[:,1], c = y2) \nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\n\nfig = draw_line(p2.w, -4, 12)\n\n\n\n\nVisualization of the perceptron’s resulting linear separator\n\nfig = plt.plot(p2.history) \nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\nVisualization of the number of iterations it took the perceptron algorithm to converge\n\n\nNon-linearly Separable Example\n\nweight3 = 0\nhistory3 = [] \nmax_steps = 10000\n\np3 = Perceptron(weight,history)\n\nnp.random.seed(12345)\n\nn3 = 100 #the number of data points\np_features3 = 4 #the number of features ends up being 2 since we do n_features = p_features - 1\n\n\nX3, y3 = make_blobs(n_samples = n3, n_features = p_features3 - 1, centers = [(-1, -1.7), (1.23, 1.7)])\nX_3 = np.append(X3, np.ones((X3.shape[0], 1)), 1)\n\n\np3.fit(X3,y3, max_steps)\n\n\nprint(p3.history[-10:])\nprint(p3.w)\n\n[0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98]\n[ 1.436715    4.80069223 -0.74920064]\n\n\nThe score did not reach 1.0, so the perceptron algorithm wasn’t able to converge signifying that the data is not linearly separable.\n\nfig = plt.scatter(X3[:,0], X3[:,1], c = y2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\n\nfig = draw_line(p3.w, -4, 4)\n\n\n\n\nVisualization of the perceptron’s resulting linear separator\n\nfig = plt.plot(p3.history) #plot is not representative of iterations (no such thing as .5 iterations) #gotta fix somehow w/ matplotlib adjusment probably\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nVisualization of the number of iterations it took the perceptron algorithm to converge\n\n\nMulti-Dimensional Example\n\nweight4 = 0\nhistory4 = [] \nmax_steps = 10000\np4 = Perceptron(weight,history)\n\nnp.random.seed(12345)\n\nn4 = 100 #the number of data points\np_features4 = 6 #the number of features ends up being 5 since we do n_features = p_features4 - 1 in make_blobs() call below\n\n\nX4, y4 = make_blobs(n_samples = n4, n_features = p_features4 - 1, centers = [(-1, -1.7,-1.5, 0, 0), (1.24, 1.7,1, 0, 0)]) #need to adjust centers to 5-dimensional space\nX_4 = np.append(X4, np.ones((X4.shape[0], 1)), 1)\n\nprint(X4.shape) #to check that our data is multi-dimensional (5-dimensions in this case) as intended, meaning there are 5 features recorded for each sample.\n\np4.fit(X4,y4, max_steps)\n\n\nprint(p4.history[-10:])\nprint(p4.w)\n\n(100, 5)\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n[4.13255343 3.42143787 1.49084004 0.05145049 0.44583831 0.3426921 ]\n\n\nIn this example the perceptron algorithm reaches a score of 1.0, meaning it was able to succesfully linearly separate the data, even in more than 2 dimensions (5 in this experiment).\n\nfig = plt.plot(p4.history) #plot is not representative of iterations (no such thing as .5 iterations) #gotta fix somehow w/ matplotlib adjusment probably\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nVisualization of the number of iterations it took the perceptron algorithm to converge\n\n\nRuntime complexity of perceptron udpate rule\nThe runtime complexity of a single iteration of the perceptron update rule is O(n) where n represents our data’s number of features. To perfom the perceptron update rule we must compute the dot product of the parameter vector and one sample vector from our modified feature matrix both of which have an entry size that’s dependent on the data’s number of features, which has multiplication and addition as its relevant operations. Our data’s number of samples doesn’t affect the time complexity of a single iteration since we only end up working with one sample (chosen randomly) at the time of the update and picking and accessing a sample in the feature matrix takes constant time."
  },
  {
    "objectID": "LogisticRegressionNotebook.html",
    "href": "LogisticRegressionNotebook.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n# fit the model\nweight = 0\nhistory = []\ns_hist = []\nLR = LogisticRegression(weight, history, s_hist)\nLR.fit(X, y, alpha = 0.1, max_epochs = 200) #alpha is the size of our steps; #epochs is our number of steps; trade-off between the two interesting\n\n# inspect the fitted value of w\nprint(LR.w) \n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\n\nfig = draw_line(LR.w, -4, 4)\n\n[0.06497765 0.07429247 0.32924044]\n[ 8.42320655  9.62149191 -1.34539919]\n\n\n/Users/hedavamsolano/Documents/GitHub/Hedavam.github.io/lr.py:154: RuntimeWarning: divide by zero encountered in log\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n/Users/hedavamsolano/Documents/GitHub/Hedavam.github.io/lr.py:154: RuntimeWarning: invalid value encountered in multiply\n  return -y*np.log(self.sigmoid(y_hat)) - (1-y)*np.log(1-self.sigmoid(y_hat))\n\n\n\n\n\n\nprint(LR.loss_history[-10:])\nprint(LR.score_history[-10:])\n\n[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]\n[0.935, 0.935, 0.935, 0.935, 0.935, 0.935, 0.935, 0.935, 0.935, 0.935]\n\n\n\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 200)\n\n[0.92068089 1.35089681 0.18943559]\n[0.54243447 0.83057858 0.99252628]\n\n\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\n\nfig = draw_line(LR.w, -2, 2)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "A blog post of an implementation of Perceptron with experiments\n\n\n\n\n\n\nMar 6, 2023\n\n\nHedavam Solano\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]