[
  {
    "objectID": "posts/Optimization for Logistic Regression/index.html",
    "href": "posts/Optimization for Logistic Regression/index.html",
    "title": "Optimization for Logistic Regression Blog",
    "section": "",
    "text": "The implementation of the perceptron algorithm could be found here: https://github.com/Hedavam/Hedavam.github.io/blob/main/posts/Optimization%20for%20Logistic%20Regression/lr.py\nThis article was helpful for drafting this blog post: https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a"
  },
  {
    "objectID": "posts/Optimization for Logistic Regression/index.html#batch",
    "href": "posts/Optimization for Logistic Regression/index.html#batch",
    "title": "Optimization for Logistic Regression Blog",
    "section": "Batch",
    "text": "Batch\nBatch gradient descent makes use of all the training data (every epoch), computes the gradient for each sample and uses the average of these gradients for the actual gradient step as it subtracts this average gradient from the parameters for however many epochs (iterations) are needed for convergence or however many epochs the user specifies as the max (hyperparameter). There’s also another hyperparameter, the learning rate, which adjusts our step size. Mathematically, if our step size is small enough, this algorithm will converge."
  },
  {
    "objectID": "posts/Optimization for Logistic Regression/index.html#stochastic",
    "href": "posts/Optimization for Logistic Regression/index.html#stochastic",
    "title": "Optimization for Logistic Regression Blog",
    "section": "Stochastic",
    "text": "Stochastic\nStochastic gradient descent also makes use of all the training data (every epoch), but divides it into batches to compute gradients and take steps with this gradient for this batches. This process is repeated for the remaining batches of the training data. The hypermarameter in this algorithm is the size of the batches.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n\nfrom sklearn.datasets import make_blobs\nfrom lr import LogisticRegression"
  },
  {
    "objectID": "posts/Optimization for Logistic Regression/index.html#speed-comparison-for-batch-vs.-stochastic-gradient-descent",
    "href": "posts/Optimization for Logistic Regression/index.html#speed-comparison-for-batch-vs.-stochastic-gradient-descent",
    "title": "Optimization for Logistic Regression Blog",
    "section": "Speed comparison for batch vs. stochastic gradient descent",
    "text": "Speed comparison for batch vs. stochastic gradient descent\n\nLR = LogisticRegression(weight, empirical_risk_history, score_hist)\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10)  #small batch size converges faster; does a lot more updates (in inner loop)?\n\nnum_steps = len(LR.loss_history)\nprint(num_steps)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression(weight, empirical_risk_history, score_hist)\nLR.fit(X, y, alpha = .01, max_epochs = 1000) #.005 and 10000 converges nearly every time; .01 and 1000 converges; #.01 is too big for 1000 epochs\n\nnum_steps = len(LR.loss_history)\nprint(num_steps)\n\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\nConverged\n12\nConveged\n570\n\n\n\n\n\nVisualization of the number of iterations it took linear regression algorithm to converge for batch vs. stochastic gradient descent\n\nLR = LogisticRegression(weight, empirical_risk_history, score_hist)\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10)  #small batch size converges faster; does a lot more updates (in inner loop)?\nnum_steps = len(LR.loss_history)\nprint(num_steps)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n\nLR = LogisticRegression(weight, empirical_risk_history, score_hist)\nLR.fit(X, y, alpha = .01, max_epochs = 1000) #.005 and 10000 converges nearly every time; .01 and 1000 converges; #.01 is too big for 1000 epochs\nnum_steps = len(LR.loss_history)\nprint(num_steps)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient (small lr)\")\n\n\nLR = LogisticRegression(weight, empirical_risk_history, score_hist)\nLR.fit(X, y, alpha = .5, max_epochs = 1000) #.005 and 10000 converges nearly every time; .01 and 1000 converges; #.01 is too big for 1000 epochs\nnum_steps = len(LR.loss_history)\nprint(num_steps)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient (big lr)\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\nConverged\n3\nConveged\n542\n1000\n\n\n\n\n\n\nTake-away 1: Stochastic gradient descent converges faster than batch gradient descent because it updates the parameters more frequently.\nFor stochastic gradient descent, in one epoch, we take many gradient steps (one for each batch) while for batch gradient descent we only take one gradient step for every epoch.\n\n\nTake-away 2: Too large of a learning rate will prevent linear regresion model optimized with gradient descent from converging."
  },
  {
    "objectID": "posts/Classifying Palmer Penguins - ML Workflow/index.html",
    "href": "posts/Classifying Palmer Penguins - ML Workflow/index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\ntrain.head(10)\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      NaN\n      7.96621\n      -25.69327\n      NaN\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      FEMALE\n      7.63220\n      -25.46569\n      NaN\n    \n    \n      2\n      PAL0910\n      124\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      MALE\n      9.59462\n      -25.42621\n      NaN\n    \n    \n      3\n      PAL0910\n      146\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      3650.0\n      MALE\n      9.22033\n      -26.03442\n      NaN\n    \n    \n      4\n      PAL0708\n      24\n      Chinstrap penguin (Pygoscelis antarctica)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      3800.0\n      MALE\n      9.28153\n      -24.97134\n      NaN\n    \n    \n      5\n      PAL0809\n      99\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N50A1\n      Yes\n      11/10/08\n      33.1\n      16.1\n      178.0\n      2900.0\n      FEMALE\n      9.04218\n      -26.15775\n      NaN\n    \n    \n      6\n      PAL0708\n      17\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N9A1\n      Yes\n      11/12/07\n      38.7\n      19.0\n      195.0\n      3450.0\n      FEMALE\n      9.18528\n      -25.06691\n      NaN\n    \n    \n      7\n      PAL0910\n      131\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N73A1\n      No\n      11/23/09\n      38.5\n      17.9\n      190.0\n      3325.0\n      FEMALE\n      8.98460\n      -25.57956\n      Nest never observed with full clutch.\n    \n    \n      8\n      PAL0708\n      9\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N35A1\n      Yes\n      11/27/07\n      43.3\n      13.4\n      209.0\n      4400.0\n      FEMALE\n      8.13643\n      -25.32176\n      NaN\n    \n    \n      9\n      PAL0708\n      38\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N24A2\n      Yes\n      11/16/07\n      42.2\n      18.5\n      180.0\n      3550.0\n      FEMALE\n      8.04787\n      -25.49523\n      NaN\n    \n  \n\n\n\n\n\nfrom itertools import combinations\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\n\n#add all features\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Region\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    #cols = [\"Flipper Length (mm)\", \"Body Mass (g)\", \"Island_Dream\", \"Island_Torgersen\"]\n\n \n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Body Mass (g)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Delta 15 N (o/oo)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Delta 13 C (o/oo)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Body Mass (g)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Delta 15 N (o/oo)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Delta 13 C (o/oo)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Flipper Length (mm)', 'Body Mass (g)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Flipper Length (mm)', 'Delta 15 N (o/oo)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Flipper Length (mm)', 'Delta 13 C (o/oo)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Body Mass (g)', 'Delta 15 N (o/oo)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Body Mass (g)', 'Delta 13 C (o/oo)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Body Mass (g)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Delta 15 N (o/oo)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Delta 13 C (o/oo)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Body Mass (g)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Delta 15 N (o/oo)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Delta 13 C (o/oo)']\n['Sex_FEMALE', 'Sex_MALE', 'Flipper Length (mm)', 'Body Mass (g)']\n['Sex_FEMALE', 'Sex_MALE', 'Flipper Length (mm)', 'Delta 15 N (o/oo)']\n['Sex_FEMALE', 'Sex_MALE', 'Flipper Length (mm)', 'Delta 13 C (o/oo)']\n['Sex_FEMALE', 'Sex_MALE', 'Body Mass (g)', 'Delta 15 N (o/oo)']\n['Sex_FEMALE', 'Sex_MALE', 'Body Mass (g)', 'Delta 13 C (o/oo)']\n['Sex_FEMALE', 'Sex_MALE', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Body Mass (g)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Delta 15 N (o/oo)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Delta 13 C (o/oo)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Body Mass (g)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Delta 15 N (o/oo)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Delta 13 C (o/oo)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Flipper Length (mm)', 'Body Mass (g)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Flipper Length (mm)', 'Delta 15 N (o/oo)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Flipper Length (mm)', 'Delta 13 C (o/oo)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Body Mass (g)', 'Delta 15 N (o/oo)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Body Mass (g)', 'Delta 13 C (o/oo)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n['Culmen Length (mm)', 'Culmen Depth (mm)']\n['Culmen Length (mm)', 'Flipper Length (mm)']\n['Culmen Length (mm)', 'Body Mass (g)']\n['Culmen Length (mm)', 'Delta 15 N (o/oo)']\n['Culmen Length (mm)', 'Delta 13 C (o/oo)']\n['Culmen Depth (mm)', 'Flipper Length (mm)']\n['Culmen Depth (mm)', 'Body Mass (g)']\n['Culmen Depth (mm)', 'Delta 15 N (o/oo)']\n['Culmen Depth (mm)', 'Delta 13 C (o/oo)']\n['Flipper Length (mm)', 'Body Mass (g)']\n['Flipper Length (mm)', 'Delta 15 N (o/oo)']\n['Flipper Length (mm)', 'Delta 13 C (o/oo)']\n['Body Mass (g)', 'Delta 15 N (o/oo)']\n['Body Mass (g)', 'Delta 13 C (o/oo)']\n['Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\n# this counts as 3 features because the two Clutch Completion \n# columns are transformations of a single original measurement. \n# you should find a way to automatically select some better columns\n# as suggested in the code block above\ncols = [\"Flipper Length (mm)\", \"Body Mass (g)\", \"Island_Dream\", \"Island_Torgersen\"]\n\nLR = LogisticRegression()\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n/Users/hedavamsolano/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n0.78125\n\n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n0.6911764705882353\n\n\n\nfrom mlxtend.plotting import plot_decision_regions\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\ndef decision_region_panel(X, y, model, qual_features):  \n  p = len(qual_features)\n  fig, axarr = plt.subplots(1, p, figsize=(4*p,4))\n  for i in range(p):\n\n      filler_feature_values = {2+j: 0 for j in range(p)}\n\n      filler_feature_values.update({2+i: 1})\n\n      ix = X[qual_features[i]] == 1\n\n      ax = axarr[i]\n\n      plot_decision_regions(np.array(X[ix]), y[ix], clf=model,\n                            filler_feature_values=filler_feature_values,\n                            filler_feature_ranges={2+j: 0.1 for j in range(p)},\n                            legend=2, ax=ax)\n\n      ax.set_xlabel(X.columns[0])\n      ax.set_ylabel(X.columns[1])\n\n      handles, labels = ax.get_legend_handles_labels()\n      ax.legend(handles, \n          [\"Adelie\", \"Chinstrap\", \"Gentoo\"], \n           framealpha=0.3, scatterpoints=1)\n\n  # Adding axes annotations\n  fig.suptitle(f'Accuracy = {model.score(X, y).round(3)}')\n  plt.tight_layout()\n  plt.show()\n\n\nqual_features = [\"Island_Dream\", \"Island_Torgersen\"]\ndecision_region_panel(X_train[cols], y_train, LR, qual_features)\n\n/Users/hedavamsolano/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n  warnings.warn(\n/Users/hedavamsolano/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n  warnings.warn(\n\n\n\n\n\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\n\nclf = Pipeline([\n  ('feature_selection', SelectFromModel(LinearSVC(penalty='l1', dual=False))),\n  ('classification', RandomForestClassifier())\n])\nclf.fit(X_train, y_train)\nclf.score(X_train, y_train)\nclf[:-1].get_feature_names_out()\n\n/Users/hedavamsolano/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n\n\narray(['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)',\n       'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)',\n       'Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Sex_MALE'],\n      dtype=object)"
  },
  {
    "objectID": "posts/Perceptron-Blog-Post/index.html",
    "href": "posts/Perceptron-Blog-Post/index.html",
    "title": "Perceptron Blog",
    "section": "",
    "text": "Perceptron & Update Rule Overview\nGiven linearly separable data, the perceptron algorithm always converges as it correctly linearly separates the data in a finite number of steps. To do so, it produces a “good” vector of parameters (weights and bias) by first initializing the parameters (usually a random guess or 0), then looping through the given feature matrix and label array and updating these parameters to produce a better prediction on the current example by moving the parameters in the right direction if (and only if) the model’s prediction for said example is incorrect (does not match the example’s actual label).\nIn the fit() method, the perceptron update rule functions as follows:\n- 1*(((2*yi-1) * self.w@xi) <= 0) indicator function returns 1 if labels don't match and 0 if they do. \n- if labels don't match we add y (-1 or 1 representation of actual label) multiplied by xi (our sample vector) to our parameter vector in either the positive or negative direction, depending on our actual label's value; 0 or -1 (negative direction); 1 or +1 (positive direction)\nNOTE: In order to account for bias we modifiy our given feature matrix by adding column of 1’s and expand our weight vector to have an extra column (to store bias in).\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\nfrom perceptron import Perceptron\n\n\n\nLinearly Separable Example 1\n\nweight = 0\nhistory = [] \nmax_steps = 10000\n\np = Perceptron(weight,history)\n\nnp.random.seed(12345)\n\nn = 100 #the number of data points\np_features = 3 #the number of features ends up being 2 since we do n_features = p_features - 1\n\n\nX, y = make_blobs(n_samples = n, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n\n\n\np.fit(X,y, max_steps)\n\n\n\n\nprint(p.history[-10:])\nprint(p.w)\n\n\n\n\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n[2.10557404 3.1165449  0.25079936]\n\n\nIn this example the perceptron algorithm reaches a score of 1.0, meaning it was able to succesfully linearly separate the data.\n\nfig = plt.scatter(X[:,0], X[:,1], c = y) #means [ first_row:last_row , column_0]\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\n\nfig = draw_line(p.w, -4, 4)\n\n\n\n\nVisualization of the perceptron’s resulting linear separator\n\nfig = plt.plot(p.history) \nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\nVisualization of the number of iterations it took the perceptron algorithm to converge\n\n\nLinearly Separable Example 2\n\nweight2 = 0\nhistory2 = [] \nmax_steps = 1000000\n\np2 = Perceptron(weight,history)\n\nnp.random.seed(12345)\n\nn2 = 100 #the number of data points\np_features2 = 3 #the number of features ends up being 2 since we do n_features = p_features - 1\n\n\nX2, y2 = make_blobs(n_samples = n2, n_features = p_features2 - 1, centers = [(-1, -1.888), (10, 1.7)])\nX_2 = np.append(X2, np.ones((X2.shape[0], 1)), 1)\n\n\n\n\np2.fit(X2,y2, max_steps)\n\n\n\n#print(p2.score(X2,y2))\n\n\nprint(p2.history[-10:])\nprint(p2.w)\n\n\n[0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 1.0]\n[ 0.25738542  0.90742861 -1.74920064]\n\n\nIn this example the perceptron algorithm reaches a score of 1.0, meaning it was able to succesfully linearly separate the data.\n\nfig = plt.scatter(X2[:,0], X2[:,1], c = y2) \nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\n\nfig = draw_line(p2.w, -4, 12)\n\n\n\n\nVisualization of the perceptron’s resulting linear separator\n\nfig = plt.plot(p2.history) \nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\nVisualization of the number of iterations it took the perceptron algorithm to converge\n\n\nNon-linearly Separable Example\n\nweight3 = 0\nhistory3 = [] \nmax_steps = 10000\n\np3 = Perceptron(weight,history)\n\nnp.random.seed(12345)\n\nn3 = 100 #the number of data points\np_features3 = 4 #the number of features ends up being 2 since we do n_features = p_features - 1\n\n\nX3, y3 = make_blobs(n_samples = n3, n_features = p_features3 - 1, centers = [(-1, -1.7), (1.23, 1.7)])\nX_3 = np.append(X3, np.ones((X3.shape[0], 1)), 1)\n\n\np3.fit(X3,y3, max_steps)\n\n\nprint(p3.history[-10:])\nprint(p3.w)\n\n[0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98]\n[ 1.436715    4.80069223 -0.74920064]\n\n\nThe score did not reach 1.0, so the perceptron algorithm wasn’t able to converge signifying that the data is not linearly separable.\n\nfig = plt.scatter(X3[:,0], X3[:,1], c = y2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\n\nfig = draw_line(p3.w, -4, 4)\n\n\n\n\nVisualization of the perceptron’s resulting linear separator\n\nfig = plt.plot(p3.history) #plot is not representative of iterations (no such thing as .5 iterations) #gotta fix somehow w/ matplotlib adjusment probably\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nVisualization of the number of iterations it took the perceptron algorithm to converge\n\n\nMulti-Dimensional Example\n\nweight4 = 0\nhistory4 = [] \nmax_steps = 10000\np4 = Perceptron(weight,history)\n\nnp.random.seed(12345)\n\nn4 = 100 #the number of data points\np_features4 = 6 #the number of features ends up being 5 since we do n_features = p_features4 - 1 in make_blobs() call below\n\n\np_features10 = 3 #gives us 10 n_features\nX10, y10 = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-0.7, -1, 0.2, 0.1, 2 , 0.4, 0.3 , 2, 0.9, 1), (0.7, 1, -0.2, -0.1, -0.3, -0.4, -0.3 , -0.6, -0.9, -1)])\n\nprint(X10.shape) #to check that our data is multi-dimensional (5-dimensions in this case) as intended, meaning there are 5 features recorded for each sample.\n\np4.fit(X10,y10, max_steps)\n\n\nprint(p4.history[-10:])\nprint(p4.w)\n\n(200, 10)\n[0.995, 0.995, 0.995, 0.995, 0.995, 0.995, 0.995, 0.995, 0.995, 1.0]\n[ 3.01593611  4.9321606   1.38926737  2.26108177 -3.32174363 -2.79359171\n -2.39895371 -5.18644807 -1.71482216 -5.7152505   7.2800442 ]\n\n\nIn this example the perceptron algorithm reaches a score of 1.0, meaning it was able to succesfully linearly separate the data, even in more than 2 dimensions (5 in this experiment).\n\nfig = plt.plot(p4.history) #plot is not representative of iterations (no such thing as .5 iterations) #gotta fix somehow w/ matplotlib adjusment probably\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nVisualization of the number of iterations it took the perceptron algorithm to converge\n\n\nRuntime complexity of perceptron udpate rule\nThe runtime complexity of a single iteration of the perceptron update rule is O(n) where n represents our data’s number of features. To perfom the perceptron update rule we must compute the dot product of the parameter vector and one sample vector from our modified feature matrix both of which have an entry size that’s dependent on the data’s number of features, which has multiplication and addition as its relevant operations. Our data’s number of samples doesn’t affect the time complexity of a single iteration since we only end up working with one sample (chosen randomly) at the time of the update and picking and accessing a sample in the feature matrix takes constant time."
  },
  {
    "objectID": "posts/Linear Regression/Untitled.html",
    "href": "posts/Linear Regression/Untitled.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import numpy as np\n\ndef pad(X):\n        return np.append(X, np.ones((X.shape[0], 1)), 1)\n\nX = np.random.rand(10, 3)\nX = pad(X)\nw = np.random.rand(X.shape[1])\n\ny = X@w + np.random.randn(X.shape[0])\n\n\ndef predict(X,y):\n    return X@w\n    \ndef score(X, y, w):"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "PerceptronNotebook.html",
    "href": "PerceptronNotebook.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Perceptron & Update Rule Overview\nGiven linearly separable data, the perceptron algorithm always converges as it correctly linearly separates the data in a finite number of steps. To do so, it produces a “good” vector of parameters (weights and bias) by first initializing the parameters (usually a random guess or 0), then looping through the given feature matrix and label array and updating these parameters to produce a better prediction on the current example (mathematical formula moves parameters in the right direction) if (and only if) the model’s prediction for said example is incorrect (does not match the example’s actual label).\n\n\nLinearly Separable Example 1\n\nweight = 0\nhistory = [] \nmax_steps = 10000\n\np = Perceptron(weight,history)\n\nnp.random.seed(12345)\n\nn = 100 #the number of data points\np_features = 3 #the number of features ends up being 2 since we do n_features = p_features - 1\n\n\nX, y = make_blobs(n_samples = n, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n\n\n\np.fit(X,y, max_steps)\n\n\n\n\nprint(p.history[-10:])\nprint(p.w)\n\n\n\n\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n[2.10557404 3.1165449  0.25079936]\n\n\nIn this example the perceptron algorithm reaches a score of 1.0, meaning it was able to succesfully linearly separate the data.\n\nfig = plt.scatter(X[:,0], X[:,1], c = y) #means [ first_row:last_row , column_0]\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\n\nfig = draw_line(p.w, -2, 2)\n\n\n\n\nVisualization of the perceptron’s resulting linear separator\n\nfig = plt.plot(p.history) \nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\nVisualization of the number of iterations it took the perceptron algorithm to converge\n\n\nLinearly Separable Example 2\n\nweight2 = 0\nhistory2 = [] \nmax_steps = 1000000\n\np2 = Perceptron(weight,history)\n\nnp.random.seed(12345)\n\nn2 = 100 #the number of data points\np_features2 = 3 #the number of features ends up being 2 since we do n_features = p_features - 1\n\n\nX2, y2 = make_blobs(n_samples = n2, n_features = p_features2 - 1, centers = [(-1, -1.888), (10, 1.7)])\nX_2 = np.append(X2, np.ones((X2.shape[0], 1)), 1)\n\n\n\n\np2.fit(X2,y2, max_steps)\n\n\n\n#print(p2.score(X2,y2))\n\n\nprint(p2.history[-10:])\nprint(p2.w)\n\n\n[0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 1.0]\n[ 0.25738542  0.90742861 -1.74920064]\n\n\nIn this example the perceptron algorithm reaches a score of 1.0, meaning it was able to succesfully linearly separate the data.\n\nfig = plt.scatter(X2[:,0], X2[:,1], c = y2) \nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\n\nfig = draw_line(p2.w, -2, 2)\n\n\n\n\nVisualization of the perceptron’s resulting linear separator\n\nfig = plt.plot(p2.history) \nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\nVisualization of the number of iterations it took the perceptron algorithm to converge\n\n\nNon-linearly Separable Example\n\nweight3 = 0\nhistory3 = [] \nmax_steps = 10000\n\np3 = Perceptron(weight,history)\n\nnp.random.seed(12345)\n\nn3 = 100 #the number of data points\np_features3 = 4 #the number of features ends up being 2 since we do n_features = p_features - 1\n\n\nX3, y3 = make_blobs(n_samples = n3, n_features = p_features3 - 1, centers = [(-1, -1.7), (1.23, 1.7)])\nX_3 = np.append(X3, np.ones((X3.shape[0], 1)), 1)\n\n\np3.fit(X3,y3, max_steps)\n\n\nprint(p3.history[-10:])\nprint(p3.w)\n\n[0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98]\n[ 1.436715    4.80069223 -0.74920064]\n\n\nThe score did not reach 1.0, so the perceptron algorithm wasn’t able to converge signifying that the data is not linearly separable.\n\nfig = plt.scatter(X3[:,0], X3[:,1], c = y2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\n\nfig = draw_line(p3.w, -2, 2)\n\n\n\n\nVisualization of the perceptron’s resulting linear separator\n\nfig = plt.plot(p3.history) #plot is not representative of iterations (no such thing as .5 iterations) #gotta fix somehow w/ matplotlib adjusment probably\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nVisualization of the number of iterations it took the perceptron algorithm to converge\n\n\nMulti-Dimensional Example\n\nweight4 = 0\nhistory4 = [] \nmax_steps = 10000\np4 = Perceptron(weight,history)\n\nnp.random.seed(12345)\n\nn4 = 100 #the number of data points\np_features4 = 4 #the number of features ends up being 3 since we do n_features = p_features - 1\n\n\nX4, y4 = make_blobs(n_samples = n4, n_features = p_features4 - 1, centers = [(-1, -1.7,-1.5), (1.24, 1.7,1)]) #need to adjust centers to 5-dimensional space\nX_4 = np.append(X4, np.ones((X4.shape[0], 1)), 1)\n\n\n#print(X4[1]) #to check that our feature matrix is multi-dimensional as intended\n\np4.fit(X4,y4, max_steps)\n\n\nprint(p4.history[-10:])\nprint(p4.w)\n\n[0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 1.0]\n[2.77694353 2.22981384 1.26393377 1.10383099]\n\n\nIn this example the perceptron algorithm reaches a score of 1.0, meaning it was able to succesfully linearly separate the data, even in more than 2 dimensions (3 in this experiment).\n\nfig = plt.plot(p4.history) #plot is not representative of iterations (no such thing as .5 iterations) #gotta fix somehow w/ matplotlib adjusment probably\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nVisualization of the number of iterations it took the perceptron algorithm to converge\n\n\nRuntime complexity of perceptron udpate rule\nThe runtime complexity of a single iteration of the perceptron update rule is O(n)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "A blog post of an implementation of Optimization for Logistic Regression with experiments\n\n\n\n\n\n\nOct 6, 2023\n\n\nHedavam Solano\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post of an implementation of Perceptron with experiments\n\n\n\n\n\n\nMar 6, 2023\n\n\nHedavam Solano\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]