[
  {
    "objectID": "posts/Optimization for Logistic Regression/index.html",
    "href": "posts/Optimization for Logistic Regression/index.html",
    "title": "Optimization for Logistic Regression Blog",
    "section": "",
    "text": "The implementation of the perceptron algorithm could be found here: https://github.com/Hedavam/Hedavam.github.io/blob/main/posts/Optimization%20for%20Logistic%20Regression/lr.py\nThis article was helpful for drafting this blog post: https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a"
  },
  {
    "objectID": "posts/Optimization for Logistic Regression/index.html#batch",
    "href": "posts/Optimization for Logistic Regression/index.html#batch",
    "title": "Optimization for Logistic Regression Blog",
    "section": "Batch",
    "text": "Batch\nBatch gradient descent makes use of all the training data (every epoch), computes the gradient for each sample and uses the average of these gradients for the actual gradient step as it subtracts this average gradient from the parameters for however many epochs (iterations) are needed for convergence or however many epochs the user specifies as the max (hyperparameter). There’s also another hyperparameter, the learning rate, which adjusts our step size. Mathematically, if our step size is small enough, this algorithm will converge."
  },
  {
    "objectID": "posts/Optimization for Logistic Regression/index.html#stochastic",
    "href": "posts/Optimization for Logistic Regression/index.html#stochastic",
    "title": "Optimization for Logistic Regression Blog",
    "section": "Stochastic",
    "text": "Stochastic\nStochastic gradient descent also makes use of all the training data (every epoch), but divides it into batches to compute gradients and take steps with this gradient for this batches. This process is repeated for the remaining batches of the training data. The hypermarameter in this algorithm is the size of the batches.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n\nfrom sklearn.datasets import make_blobs\nfrom optimization_logistic_regression import LogisticRegression"
  },
  {
    "objectID": "posts/Optimization for Logistic Regression/index.html#speed-comparison-for-batch-vs.-stochastic-gradient-descent",
    "href": "posts/Optimization for Logistic Regression/index.html#speed-comparison-for-batch-vs.-stochastic-gradient-descent",
    "title": "Optimization for Logistic Regression Blog",
    "section": "Speed comparison for batch vs. stochastic gradient descent",
    "text": "Speed comparison for batch vs. stochastic gradient descent\n\nLR = LogisticRegression(weight, empirical_risk_history, score_hist)\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10)  #small batch size converges faster; does a lot more updates (in inner loop)?\n\nnum_steps = len(LR.loss_history)\nprint(num_steps)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression(weight, empirical_risk_history, score_hist)\nLR.fit(X, y, alpha = .01, max_epochs = 1000) #.005 and 10000 converges nearly every time; .01 and 1000 converges; #.01 is too big for 1000 epochs\n\nnum_steps = len(LR.loss_history)\nprint(num_steps)\n\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\nConverged\n12\nConveged\n570\n\n\n\n\n\nVisualization of the number of iterations it took linear regression algorithm to converge for batch vs. stochastic gradient descent\n\nLR = LogisticRegression(weight, empirical_risk_history, score_hist)\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10)  #small batch size converges faster; does a lot more updates (in inner loop)?\nnum_steps = len(LR.loss_history)\nprint(num_steps)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n\nLR = LogisticRegression(weight, empirical_risk_history, score_hist)\nLR.fit(X, y, alpha = .01, max_epochs = 1000) #.005 and 10000 converges nearly every time; .01 and 1000 converges; #.01 is too big for 1000 epochs\nnum_steps = len(LR.loss_history)\nprint(num_steps)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient (small lr)\")\n\n\nLR = LogisticRegression(weight, empirical_risk_history, score_hist)\nLR.fit(X, y, alpha = .5, max_epochs = 1000) #.005 and 10000 converges nearly every time; .01 and 1000 converges; #.01 is too big for 1000 epochs\nnum_steps = len(LR.loss_history)\nprint(num_steps)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient (big lr)\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\nConverged\n3\nConveged\n542\n1000\n\n\n\n\n\n\nTake-away 1: Stochastic gradient descent converges faster than batch gradient descent because it updates the parameters more frequently.\nFor stochastic gradient descent, in one epoch, we take many gradient steps (one for each batch) while for batch gradient descent we only take one gradient step for every epoch.\n\n\nTake-away 2: Too large of a learning rate will prevent linear regresion model optimized with gradient descent from converging."
  },
  {
    "objectID": "posts/Classifying Palmer Penguins - ML Workflow/index.html",
    "href": "posts/Classifying Palmer Penguins - ML Workflow/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Import appropriate dataset for training\n\n“The Palmer Penguins data set is a data set collected by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network. The data contains physiological measurements for a number of individuals from each of three species of penguin:”\nQuote taken from: https://middlebury-csci-0451.github.io/CSCI-0451/assignments/blog-posts/blog-post-penguins.html#resources-and-hints\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head(5)\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      NaN\n      7.96621\n      -25.69327\n      NaN\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      FEMALE\n      7.63220\n      -25.46569\n      NaN\n    \n    \n      2\n      PAL0910\n      124\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      MALE\n      9.59462\n      -25.42621\n      NaN\n    \n    \n      3\n      PAL0910\n      146\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      3650.0\n      MALE\n      9.22033\n      -26.03442\n      NaN\n    \n    \n      4\n      PAL0708\n      24\n      Chinstrap penguin (Pygoscelis antarctica)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      3800.0\n      MALE\n      9.28153\n      -24.97134\n      NaN\n    \n  \n\n\n\n\n\n\n\nExplore training data with tables and visuals\nNOTE: For convenience, we will be making our tables and visuals by manipulating our original penguins dataframe as opposed to working with modified training data.\n\nTables\n\nUseful Resources:\nhttps://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.aggregate.html\nFor tables, we will use pandas’s groupby and aggregate methods.\nThe Pandas dataframe.groupby() method allows us to split up the dataframe into groups based on certain criteria.\nThe Pandas dataframe.aggregate() or dataframe.agg() method allows us to perform a variety of aggregation functions on a dataframe.\nChaining these 2 methods, we can split up a dataframe into desired groups and peform desired aggregation functions on this specially grouped dataframe.\nThe table below groups by Island and Species and finds the mean Body Mass (g) and Flipper Length (mm) for each group.\nFrom this table, we make a few interesting discoveries: - Adelie penguins are present in all 3 islands - The Body Mass (g) and Flipper Length (mm) of Adelie penguins is consistent across all 3 Islands - Gentoo penguins are only present in Biscoe Island - Chinstrap penguins are only present in Dream Island\n\ntrain.groupby([\"Island\", \"Species\"]).agg({\"Body Mass (g)\":'mean', \"Flipper Length (mm)\":'mean'})\n\n\n\n\n\n  \n    \n      \n      \n      Body Mass (g)\n      Flipper Length (mm)\n    \n    \n      Island\n      Species\n      \n      \n    \n  \n  \n    \n      Biscoe\n      Adelie Penguin (Pygoscelis adeliae)\n      3648.571429\n      188.714286\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      5141.666667\n      217.729167\n    \n    \n      Dream\n      Adelie Penguin (Pygoscelis adeliae)\n      3674.375000\n      189.900000\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      3717.857143\n      195.464286\n    \n    \n      Torgersen\n      Adelie Penguin (Pygoscelis adeliae)\n      3693.918919\n      191.810811\n    \n  \n\n\n\n\nThe table below groups by Sex and Species and finds the mean Body Mass (g) and Flipper Length (mm) for each group.\nFrom this table, we make a few interesting discoveries: - Male penguins weigh more and have longer flippers than their female counterpants for all 3 species - Gentoo penguins are significantly heavier have longer flippers than the other 2 species, regardless of sex - Despite being similar in weight, Chinstrap penguins have longer flippers than Adelie penguins - The differences in weight between Male and Female penguins of the same species follows this ascending order: Chinstrap, Adelie, Gentoo\n\ntrain.groupby([\"Sex\", \"Species\"]).agg({\"Body Mass (g)\":'mean', \"Flipper Length (mm)\":'mean'})\n\n\n\n\n\n  \n    \n      \n      \n      Body Mass (g)\n      Flipper Length (mm)\n    \n    \n      Sex\n      Species\n      \n      \n    \n  \n  \n    \n      FEMALE\n      Adelie Penguin (Pygoscelis adeliae)\n      3337.280702\n      187.719298\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      3514.655172\n      191.551724\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      4677.976190\n      212.928571\n    \n    \n      MALE\n      Adelie Penguin (Pygoscelis adeliae)\n      4020.454545\n      192.690909\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      3936.111111\n      199.666667\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      5502.314815\n      221.462963\n    \n  \n\n\n\n\n\n\n\nVisuals\n\nUseful Resource:\nhttps://seaborn.pydata.org/tutorial/introduction.html\nFor visuals, we will use seaborn. Seaborn, as paraphrased from their documentation, is a library for graphing in Python that builds on matplotlib and integrates with pandas data structures.\nThe visualization below is a categorical scatterplot with sex on the categorical axis (y-axis in this case) and the numerical variable culmen depth on the x-axis. The points, representing individual penguins, are colored in correspondence to the penguin’s species.\nFrom this visualization, we make a few interesting discoveries:\n\nMale penguins have a larger culmen depth than their female counterparts for all 3 species\nFor both sexes, Gentoo penguins have a relatively low culmen depth that the other species don’t share, which may prove useful for classification\n\n\nimport seaborn as sns\nsns.set_theme(style=\"whitegrid\", palette=\"muted\")\n\n\ntrain = train.query(\"Sex == 'MALE' or Sex == 'FEMALE'\") #removes penguins with NaN entries in the Sex column\n\nax = sns.swarmplot(data=train, x=\"Culmen Depth (mm)\", y= \"Sex\", hue=\"Species\")\n\n\n\n\nThe visualization below is the same as the one above but culmen depth has been replaced with culmen length.\nFrom this visualization, we make a few interesting discoveries:\n\nMale penguins have a larger culmen length than their female counterparts for all 3 species\nFor both sexes, Adelie penguins have a relatively low culmen depth that the other species don’t share, which may prove useful for classification\n\n\nimport seaborn as sns\nsns.set_theme(style=\"whitegrid\", palette=\"muted\")\n\n\nax = sns.swarmplot(data=train, x=\"Culmen Length (mm)\", y= \"Sex\", hue=\"Species\")\n\n\n\n\nThe visualization below is a scatter plot analyzing the relationship between 2 numerical variables. Namely culmen depth (y-axis) and culmen length (x-axis). There is also a linear regression model overlayed on each subet of points corresponding to one of the three species.\nFrom this visualization, we make a few interesting discoveries:\n\nFor all penguins, there’s a positive correlation between culmen length and culmen depth, though it’s notably not as strong for Adelie penguins\nAdelie and Chinstrap penguins have overlapping culmen depths\nGentoo and Chinstrap penguins have overlapping culmen lenghts\nEach species lives in its own section of the visualization, which may prove useful for classification\n\n\nsns.lmplot(data=train, x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", hue=\"Species\")\n\n<seaborn.axisgrid.FacetGrid at 0x7fa7c1705eb0>\n\n\n\n\n\n\n\n\n\nPrepare the data for model training\nBefore training, we must construct our feature matrix and target vector.\nFor the feature matrix, we must modify our qualitative measures into numerical ones. For example, the Sex column in our train dataframe with Male or Female as possible entries needs to be adjusted to two columns (Sex_Male & Sex_Female) with 0s or 1s as possible entries for these columns.\nFor the target vector, we use sklearn’s LabelEncoder to turn our Species column w/ possible entries being the strings “Adelie”, “Chinstrap”, “Gentoo” being converted into 0,1,2 respectively (it appears that LabelEncoder assigns its indexes alphabetically).\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder() \nle.fit(train[\"Species\"]) #Fit label encoder.\n\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1) #drops \"irrelevant features\"\n  df = df[df[\"Sex\"] != \".\"] #filters Sex to be binary\n  df = df.dropna() #drops NA Values\n  y = le.transform(df[\"Species\"]) #Transform labels to normalized encoding\n  df = df.drop([\"Species\"], axis = 1) #drops Species from our feature matrix since it will be our target vector\n  df = pd.get_dummies(df) #Encodes qualitative features as numerical ones\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nX_train.head(5) #resulting feature matrix for training\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n\n\n\n\ny_train #resulting target vector\n\narray([2, 0, 0, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 2, 1, 2, 2, 0, 0, 1,\n       2, 2, 0, 2, 0, 1, 1, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n       1, 0, 2, 2, 2, 1, 1, 1, 2, 2, 2, 0, 2, 0, 2, 2, 2, 0, 2, 1, 0, 0,\n       2, 0, 2, 2, 0, 2, 0, 0, 2, 1, 1, 2, 2, 0, 1, 2, 2, 2, 1, 0, 1, 0,\n       0, 0, 0, 1, 2, 0, 2, 0, 0, 2, 0, 2, 2, 0, 0, 1, 0, 2, 0, 2, 0, 2,\n       0, 0, 2, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0, 2, 2,\n       0, 2, 2, 0, 1, 2, 1, 2, 0, 0, 0, 2, 0, 0, 1, 1, 0, 2, 1, 2, 2, 2,\n       2, 0, 2, 0, 0, 0, 0, 1, 0, 0, 2, 2, 0, 2, 0, 1, 0, 1, 2, 1, 1, 1,\n       2, 0, 1, 0, 0, 1, 0, 0, 2, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 2, 1, 2,\n       0, 1, 2, 0, 2, 2, 1, 1, 0, 2, 0, 0, 2, 1, 0, 2, 2, 1, 1, 2, 2, 2,\n       0, 2, 2, 2, 2, 0, 0, 1, 2, 1, 2, 2, 1, 2, 0, 1, 0, 0, 0, 0, 1, 2,\n       0, 1, 1, 0, 2, 0, 1, 1, 1, 0, 2, 0, 2, 2])\n\n\n\n\nChoose features based on model success on training data\n\nCross-validation\nThe accuracy of our model on training data is not neccessarily a good representation of how it may perform on test data. We can use cross-validatino to test our model’s ability to generalize (perform well on unseen data). We split up our training data into groups and setting one of these groups as pseudo-testing data while we train the model on the rest of the groups. Cross-validation can be useful for hyperparameter tuning.\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(clf, X_train, y_train, cv=5)\ncv_scores\n\ncv_scores.mean()\n\nfor i in range(1,10):\n    clf = RandomForestClassifier(max_depth = i, random_state=0)\n    cv_scores = cross_val_score(clf, X_train, y_train, cv=5)\n    print(\"Max_depth = \" + str(i) + \": \" + str(cv_scores.mean()))\n\nMax_depth = 1: 0.7812971342383107\nMax_depth = 2: 0.9726998491704373\nMax_depth = 3: 0.984389140271493\nMax_depth = 4: 0.984389140271493\nMax_depth = 5: 0.9921568627450981\nMax_depth = 6: 0.9921568627450981\nMax_depth = 7: 0.9921568627450981\nMax_depth = 8: 0.9921568627450981\nMax_depth = 9: 0.9921568627450981\n\n\nOur cross_validation score plateus after max_depth = 5, so we should use a max_depth close to 5 when trying to find the features that our model performs the best on (highest score).\nNote: For this specific blog post, we want our model’s score on our training data to be 1.\n\n\nModel\nThe model we will use is the RandomForestClassifier.\nRandom forests are basically a collection of decision trees.\nDecision trees classify data based on a number of questions about its features. First, we must decide which question to ask first at the tree’s root node. We pick the “best” possible question. Then we divide the data into subsets based on the responses to the root question. Then, we recurse and repeat the same process (choosing “best” questions on our subsets). This divide/conquer algorithm continues until there are no remaining “best” questions that improve our classification task.\nBagging trees (or bootstrap aggregating) consists of sampling from our data with replacement (bootstrapping), training various decision trees on these samples and aggregating their results. Aggregation for classification is done by taking the label that is predicted by the majority of the decision trees as our prediction.\nBagged trees with a parameter controlling the number of features used to find the best split in each tree make up a random forest classifier.\nMore info on the innerworkings of random forests: https://victorzhou.com/blog/intro-to-random-forests\n\n\nFeature Selection\nFor feature selection, we will perform an exhaustive search of all the features contained in the data set.\nFor every combination of 1 qualitative feature and 2 quantitative features, we will fit a model and calculate its score on the training data. We will select the features corresponding to the highest score.\n\nfrom itertools import combinations\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\n\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\n\n\ncol_array = [] #to store features used\nscore_array = [] #to store model scores\n\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    col_array.append(cols) \n    clf = RandomForestClassifier(max_depth = 6, random_state=0)\n    clf.fit(X_train[cols], y_train)\n    score_array.append(clf.score(X_train[cols], y_train))\n\n    \n#make data frame with 2 columns: features and scores \nd = {'features': col_array, 'scores': score_array} \nfeatures_df = pd.DataFrame(data=d)\n\nmax_index = features_df[['scores']].idxmax() #find index corresponding to highest score\nbest_features = (col_array[int(max_index)]) #find best features using max_index\n\nprint(\"The features that our model produces the highest score on are: \" + str(best_features))\n\nThe features that our model produces the highest score on are: ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Sex_FEMALE', 'Sex_MALE']\n\n\n\nfeatures_df[int(max_index):int(max_index+1)]\n\n\n\n\n\n  \n    \n      \n      features\n      scores\n    \n  \n  \n    \n      15\n      [Culmen Length (mm), Culmen Depth (mm), Sex_FE...\n      1.0\n    \n  \n\n\n\n\n\n\n\nExamine model success on validation data\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n#fit model to training data\nclf = RandomForestClassifier(max_depth=6, random_state=0)\nclf.fit(X_train[best_features], y_train)\n\n\n#get score on validation data\nX_test, y_test = prepare_data(test)\nclf.score(X_test[best_features], y_test)\n\n0.9705882352941176\n\n\nOur model does an excellent job at classifiying penguins by species, though it isn’t able to achieve 100% accuracy on test data as it did for training data.\n\n\nConcluding Remarks & Visualization\n\nfrom mlxtend.plotting import plot_decision_regions\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:] #female & male\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n        \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n    \n      axarr[0].set(title = \"Female\")\n      axarr[1].set(title = \"Male\")\n    \n      plt.tight_layout()\n        \n      \n        \n\n\nclf = RandomForestClassifier(max_depth=6, random_state=0)\nclf.fit(X_train[best_features], y_train)\nplot_regions(clf, X_train[best_features], y_train)\n\n\n\n\nOur visualization shows that were able to accurately classify Palmer penguins using our model, without much overfitting (our decision boundaries aren’t too “wiggly”). There might be some overfitting, especially with the female chinstrap penguin who’s culmen length and depth is very close to that of Gentoo penguins."
  },
  {
    "objectID": "posts/Perceptron-Blog-Post/index.html",
    "href": "posts/Perceptron-Blog-Post/index.html",
    "title": "Perceptron Blog",
    "section": "",
    "text": "Perceptron & Update Rule Overview\nGiven linearly separable data, the perceptron algorithm always converges as it correctly linearly separates the data in a finite number of steps. To do so, it produces a “good” vector of parameters (weights and bias) by first initializing the parameters (usually a random guess or 0), then looping through the given feature matrix and label array and updating these parameters to produce a better prediction on the current example by moving the parameters in the right direction if (and only if) the model’s prediction for said example is incorrect (does not match the example’s actual label).\nIn the fit() method, the perceptron update rule functions as follows:\n- 1*(((2*yi-1) * self.w@xi) <= 0) indicator function returns 1 if labels don't match and 0 if they do. \n- if labels don't match we add y (-1 or 1 representation of actual label) multiplied by xi (our sample vector) to our parameter vector in either the positive or negative direction, depending on our actual label's value; 0 or -1 (negative direction); 1 or +1 (positive direction)\nNOTE: In order to account for bias we modifiy our given feature matrix by adding column of 1’s and expand our weight vector to have an extra column (to store bias in).\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\nfrom perceptron import Perceptron\n\n\n\nLinearly Separable Example 1\n\nweight = 0\nhistory = [] \nmax_steps = 10000\n\np = Perceptron(weight,history)\n\nnp.random.seed(12345)\n\nn = 100 #the number of data points\np_features = 3 #the number of features ends up being 2 since we do n_features = p_features - 1\n\n\nX, y = make_blobs(n_samples = n, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n\n\n\np.fit(X,y, max_steps)\n\n\n\n\nprint(p.history[-10:])\nprint(p.w)\n\n\n\n\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n[2.10557404 3.1165449  0.25079936]\n\n\nIn this example the perceptron algorithm reaches a score of 1.0, meaning it was able to succesfully linearly separate the data.\n\nfig = plt.scatter(X[:,0], X[:,1], c = y) #means [ first_row:last_row , column_0]\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\n\nfig = draw_line(p.w, -4, 4)\n\n\n\n\nVisualization of the perceptron’s resulting linear separator\n\nfig = plt.plot(p.history) \nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\nVisualization of the number of iterations it took the perceptron algorithm to converge\n\n\nLinearly Separable Example 2\n\nweight2 = 0\nhistory2 = [] \nmax_steps = 1000000\n\np2 = Perceptron(weight,history)\n\nnp.random.seed(12345)\n\nn2 = 100 #the number of data points\np_features2 = 3 #the number of features ends up being 2 since we do n_features = p_features - 1\n\n\nX2, y2 = make_blobs(n_samples = n2, n_features = p_features2 - 1, centers = [(-1, -1.888), (10, 1.7)])\nX_2 = np.append(X2, np.ones((X2.shape[0], 1)), 1)\n\n\n\n\np2.fit(X2,y2, max_steps)\n\n\n\n#print(p2.score(X2,y2))\n\n\nprint(p2.history[-10:])\nprint(p2.w)\n\n\n[0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 1.0]\n[ 0.25738542  0.90742861 -1.74920064]\n\n\nIn this example the perceptron algorithm reaches a score of 1.0, meaning it was able to succesfully linearly separate the data.\n\nfig = plt.scatter(X2[:,0], X2[:,1], c = y2) \nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\n\nfig = draw_line(p2.w, -4, 12)\n\n\n\n\nVisualization of the perceptron’s resulting linear separator\n\nfig = plt.plot(p2.history) \nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\nVisualization of the number of iterations it took the perceptron algorithm to converge\n\n\nNon-linearly Separable Example\n\nweight3 = 0\nhistory3 = [] \nmax_steps = 10000\n\np3 = Perceptron(weight,history)\n\nnp.random.seed(12345)\n\nn3 = 100 #the number of data points\np_features3 = 4 #the number of features ends up being 2 since we do n_features = p_features - 1\n\n\nX3, y3 = make_blobs(n_samples = n3, n_features = p_features3 - 1, centers = [(-1, -1.7), (1.23, 1.7)])\nX_3 = np.append(X3, np.ones((X3.shape[0], 1)), 1)\n\n\np3.fit(X3,y3, max_steps)\n\n\nprint(p3.history[-10:])\nprint(p3.w)\n\n[0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98]\n[ 1.436715    4.80069223 -0.74920064]\n\n\nThe score did not reach 1.0, so the perceptron algorithm wasn’t able to converge signifying that the data is not linearly separable.\n\nfig = plt.scatter(X3[:,0], X3[:,1], c = y2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\n\nfig = draw_line(p3.w, -4, 4)\n\n\n\n\nVisualization of the perceptron’s resulting linear separator\n\nfig = plt.plot(p3.history) #plot is not representative of iterations (no such thing as .5 iterations) #gotta fix somehow w/ matplotlib adjusment probably\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nVisualization of the number of iterations it took the perceptron algorithm to converge\n\n\nMulti-Dimensional Example\n\nweight4 = 0\nhistory4 = [] \nmax_steps = 10000\np4 = Perceptron(weight,history)\n\nnp.random.seed(12345)\n\nn4 = 100 #the number of data points\np_features4 = 6 #the number of features ends up being 5 since we do n_features = p_features4 - 1 in make_blobs() call below\n\n\np_features10 = 3 #gives us 10 n_features\nX10, y10 = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-0.7, -1, 0.2, 0.1, 2 , 0.4, 0.3 , 2, 0.9, 1), (0.7, 1, -0.2, -0.1, -0.3, -0.4, -0.3 , -0.6, -0.9, -1)])\n\nprint(X10.shape) #to check that our data is multi-dimensional (5-dimensions in this case) as intended, meaning there are 5 features recorded for each sample.\n\np4.fit(X10,y10, max_steps)\n\n\nprint(p4.history[-10:])\nprint(p4.w)\n\n(200, 10)\n[0.995, 0.995, 0.995, 0.995, 0.995, 0.995, 0.995, 0.995, 0.995, 1.0]\n[ 3.01593611  4.9321606   1.38926737  2.26108177 -3.32174363 -2.79359171\n -2.39895371 -5.18644807 -1.71482216 -5.7152505   7.2800442 ]\n\n\nIn this example the perceptron algorithm reaches a score of 1.0, meaning it was able to succesfully linearly separate the data, even in more than 2 dimensions (5 in this experiment).\n\nfig = plt.plot(p4.history) #plot is not representative of iterations (no such thing as .5 iterations) #gotta fix somehow w/ matplotlib adjusment probably\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nVisualization of the number of iterations it took the perceptron algorithm to converge\n\n\nRuntime complexity of perceptron udpate rule\nThe runtime complexity of a single iteration of the perceptron update rule is O(n) where n represents our data’s number of features. To perfom the perceptron update rule we must compute the dot product of the parameter vector and one sample vector from our modified feature matrix both of which have an entry size that’s dependent on the data’s number of features, which has multiplication and addition as its relevant operations. Our data’s number of samples doesn’t affect the time complexity of a single iteration since we only end up working with one sample (chosen randomly) at the time of the update and picking and accessing a sample in the feature matrix takes constant time."
  },
  {
    "objectID": "posts/Linear Regression/Untitled.html",
    "href": "posts/Linear Regression/Untitled.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n#from notes\n\ndef read_titanic_data(url):\n  df = pd.read_csv(url)\n  y = df[\"Survived\"]\n  X = df.drop([\"Survived\", \"Name\"], axis = 1)\n  return df, X, y\n\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/titanic/train.csv\"\ndf_train, X_train, y_train = read_titanic_data(train_url)\n\n\ndf_train.head()\n\nX_train = pd.get_dummies(X_train, columns = [\"Sex\"], drop_first = \"if_binary\")\nX_train.head()\n\nfrom sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression()\nLR.fit(X_train, y_train)\n\n#train a model on training data\n\n\n#Get test data\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/titanic/test.csv\"\n\ndf_test, X_test, y_test = read_titanic_data(test_url)\nX_test = pd.get_dummies(X_test, columns = [\"Sex\"], drop_first=\"if_binary\")\n\n#get predictions\n\ny_pred = LR.predict(X_test)\nbase_rate_p = y_train.mean() #prevalence\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred) #for rates below\n\ntp = cm[1,1]\ntn = cm[0,0]\nfp = cm[1,0]\nfn = cm[0,1]\n\nPPV = tp/(tp + fp) #positive predictive value\nTPR = tp/(tp + fn) #true positive rate\nFNR = fn/(fn + tp) #false negative rate\n\n\nlhs_FPR = fp/(fp + tn) #false postive rate\n\ndef rhs_FPR_func(lhs_FPR):\n    \n    rhs_FPR = (base_rate_p/(1-base_rate_p)) * ((1-PPV)/PPV) * (1-FNR)\n\n    return lhs_FPR, rhs_FPR\n\nprint (rhs_FPR_func(lhs_FPR))\n\n    \n\n(0.10185185185185185, 0.10135896586012595)"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "PerceptronNotebook.html",
    "href": "PerceptronNotebook.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Perceptron & Update Rule Overview\nGiven linearly separable data, the perceptron algorithm always converges as it correctly linearly separates the data in a finite number of steps. To do so, it produces a “good” vector of parameters (weights and bias) by first initializing the parameters (usually a random guess or 0), then looping through the given feature matrix and label array and updating these parameters to produce a better prediction on the current example (mathematical formula moves parameters in the right direction) if (and only if) the model’s prediction for said example is incorrect (does not match the example’s actual label).\n\n\nLinearly Separable Example 1\n\nweight = 0\nhistory = [] \nmax_steps = 10000\n\np = Perceptron(weight,history)\n\nnp.random.seed(12345)\n\nn = 100 #the number of data points\np_features = 3 #the number of features ends up being 2 since we do n_features = p_features - 1\n\n\nX, y = make_blobs(n_samples = n, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n\n\n\np.fit(X,y, max_steps)\n\n\n\n\nprint(p.history[-10:])\nprint(p.w)\n\n\n\n\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n[2.10557404 3.1165449  0.25079936]\n\n\nIn this example the perceptron algorithm reaches a score of 1.0, meaning it was able to succesfully linearly separate the data.\n\nfig = plt.scatter(X[:,0], X[:,1], c = y) #means [ first_row:last_row , column_0]\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\n\nfig = draw_line(p.w, -2, 2)\n\n\n\n\nVisualization of the perceptron’s resulting linear separator\n\nfig = plt.plot(p.history) \nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\nVisualization of the number of iterations it took the perceptron algorithm to converge\n\n\nLinearly Separable Example 2\n\nweight2 = 0\nhistory2 = [] \nmax_steps = 1000000\n\np2 = Perceptron(weight,history)\n\nnp.random.seed(12345)\n\nn2 = 100 #the number of data points\np_features2 = 3 #the number of features ends up being 2 since we do n_features = p_features - 1\n\n\nX2, y2 = make_blobs(n_samples = n2, n_features = p_features2 - 1, centers = [(-1, -1.888), (10, 1.7)])\nX_2 = np.append(X2, np.ones((X2.shape[0], 1)), 1)\n\n\n\n\np2.fit(X2,y2, max_steps)\n\n\n\n#print(p2.score(X2,y2))\n\n\nprint(p2.history[-10:])\nprint(p2.w)\n\n\n[0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 1.0]\n[ 0.25738542  0.90742861 -1.74920064]\n\n\nIn this example the perceptron algorithm reaches a score of 1.0, meaning it was able to succesfully linearly separate the data.\n\nfig = plt.scatter(X2[:,0], X2[:,1], c = y2) \nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\n\nfig = draw_line(p2.w, -2, 2)\n\n\n\n\nVisualization of the perceptron’s resulting linear separator\n\nfig = plt.plot(p2.history) \nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\nVisualization of the number of iterations it took the perceptron algorithm to converge\n\n\nNon-linearly Separable Example\n\nweight3 = 0\nhistory3 = [] \nmax_steps = 10000\n\np3 = Perceptron(weight,history)\n\nnp.random.seed(12345)\n\nn3 = 100 #the number of data points\np_features3 = 4 #the number of features ends up being 2 since we do n_features = p_features - 1\n\n\nX3, y3 = make_blobs(n_samples = n3, n_features = p_features3 - 1, centers = [(-1, -1.7), (1.23, 1.7)])\nX_3 = np.append(X3, np.ones((X3.shape[0], 1)), 1)\n\n\np3.fit(X3,y3, max_steps)\n\n\nprint(p3.history[-10:])\nprint(p3.w)\n\n[0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98]\n[ 1.436715    4.80069223 -0.74920064]\n\n\nThe score did not reach 1.0, so the perceptron algorithm wasn’t able to converge signifying that the data is not linearly separable.\n\nfig = plt.scatter(X3[:,0], X3[:,1], c = y2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\n\nfig = draw_line(p3.w, -2, 2)\n\n\n\n\nVisualization of the perceptron’s resulting linear separator\n\nfig = plt.plot(p3.history) #plot is not representative of iterations (no such thing as .5 iterations) #gotta fix somehow w/ matplotlib adjusment probably\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nVisualization of the number of iterations it took the perceptron algorithm to converge\n\n\nMulti-Dimensional Example\n\nweight4 = 0\nhistory4 = [] \nmax_steps = 10000\np4 = Perceptron(weight,history)\n\nnp.random.seed(12345)\n\nn4 = 100 #the number of data points\np_features4 = 4 #the number of features ends up being 3 since we do n_features = p_features - 1\n\n\nX4, y4 = make_blobs(n_samples = n4, n_features = p_features4 - 1, centers = [(-1, -1.7,-1.5), (1.24, 1.7,1)]) #need to adjust centers to 5-dimensional space\nX_4 = np.append(X4, np.ones((X4.shape[0], 1)), 1)\n\n\n#print(X4[1]) #to check that our feature matrix is multi-dimensional as intended\n\np4.fit(X4,y4, max_steps)\n\n\nprint(p4.history[-10:])\nprint(p4.w)\n\n[0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 0.97, 1.0]\n[2.77694353 2.22981384 1.26393377 1.10383099]\n\n\nIn this example the perceptron algorithm reaches a score of 1.0, meaning it was able to succesfully linearly separate the data, even in more than 2 dimensions (3 in this experiment).\n\nfig = plt.plot(p4.history) #plot is not representative of iterations (no such thing as .5 iterations) #gotta fix somehow w/ matplotlib adjusment probably\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nVisualization of the number of iterations it took the perceptron algorithm to converge\n\n\nRuntime complexity of perceptron udpate rule\nThe runtime complexity of a single iteration of the perceptron update rule is O(n)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "A blog post of an implementation of Optimization for Logistic Regression with experiments\n\n\n\n\n\n\nOct 6, 2023\n\n\nHedavam Solano\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 18, 2023\n\n\nHedavam Solano\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post in which we classify Palmer Penguins given a feature-rich dataset using the standard machine learning workflow.\n\n\n\n\n\n\nMar 16, 2023\n\n\nHedavam Solano\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2023\n\n\nHedavam Solano\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post of an implementation of Optimization for Logistic Regression with experiments\n\n\n\n\n\n\nMar 13, 2023\n\n\nHedavam Solano\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post of an implementation of Perceptron with experiments\n\n\n\n\n\n\nMar 6, 2023\n\n\nHedavam Solano\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Linear Regression/index.html",
    "href": "posts/Linear Regression/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\nfrom linearRegression import LinearRegression"
  },
  {
    "objectID": "posts/Learning from Timnit Gebru/index.html",
    "href": "posts/Learning from Timnit Gebru/index.html",
    "title": "Learning from Timnit Gebru",
    "section": "",
    "text": "Dr. Gebru is an Ethiopian American computer scientist who specializes in algorithmic bias. At 15 years of age, she fled the Eritrean–Ethiopian War and lived in Ireland for a bit until her request for a U.S Visa got approved after it was denied the first time. In her new school environments, Gebru faced discrimination on the basis of being an African refugee as some of her teachers didn’t allow her to take higher level classes despite her prolific academic sucess. Furthemore, being inspired by a racist run-in with police, in which her friend had been assualted in a bar and then wrongfully arrested despite being the victim, Gebru decided to specialize int the field of ethics in technology. Her departure from Google epitimizes her work’s essence as she states on her LinkedIn that she was fired from Google “for raising issues of discrimination in the workplace.” after she disputed Google’s request to withdraw a research paper dealing with sensitive topics. Her impressive career in the tech industry that has been widely recognized as she was one of the 100 most influential people by Time magazine amongst many other honorary distinctions.\nDr. Gebru will be giving a virtual talk at Middlebury on the bias and social impacts of AI on April 24, 2023 at 7:00 PM ET in the Franklin Environmental Center at Hillcrest.\n\n\n\n\n\nWith a single point of failure (glitch), if a hiring algorithm becomes widely used, certain groups of people might fail to get jobs. The face recognition algorithm identifying protesters through their social media was used by police to target protesters for unrelated things. From these examples we can conclude that faulty AI or AI in the wrong hands can impact our society negatively.\nOn a more micro level, there are questions about the validity of the data that informs AI as it often reflects patterns of inclusion & exclusion. Diversity in datasets is not the norm. Tech companies have recognized this issue and tried to alleviate it, but visibility is not inclusion. Dr. Gebru points out many examples of tech companies using predatory methods like pulling transgender people’s videos without consent for the purposes of diversifying their datasets.\nDr. Gebru points out that an algorithm’s measure of fairness is not necessarily it’s perfomance being equal for different groups since there’s other factors like error rate balance that may prove an algorithm to be unfair.\nSurely, representation is not an AI-specific issue, but AI’s potential power may amplify the negative effects of misrepresentation.\nDr. Gebru concludes by urging us to ask questions like: - Who is funding/endorsing & developing AI models? - Usually, the dominant group - Who is negatively affected by unfairness in these models? - Usually, the marginalized groups\nLooking into the future, Dr. Gebru points out that by incorporating more people from marginalized communities in the world of AI (which is currently very homogenous), the algorithmic bias of models will be diminished.\ntl;dr: As a society, we need to be more aware of AI’s inherent biases and how it may uphold the status quo, and we should diversify the AI field to progressively eliminate algorithmic biases.\n\n\n\n\n\n\nGiven your experience as the co-leader of an ethics of AI team at Google, do you think algorithmic bias auditing should be overseen by a separate department within tech companies or would it be better for trusted independent companies to do bias auditing in order to keep big tech companies in check?\n\n\n\nOn DAIR’s website, there are 2 linked research projects (The Legacy of Spatial Apartheid and Testing and Documentation). Is DAIR working on some new projects or updating these current projects?\nWhat are some interesting findings that have been made with the visual dataset of South Africa DAIR helped create?\nWhat were some difficulties making the datasheet for the visual dataset of South Africa DAIR helped create?"
  },
  {
    "objectID": "Untitled1.html",
    "href": "Untitled1.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "data = [[1, 2],[3, 4]]\nx_data = torch.tensor(data)\n\nprint(x_data)\n\ntensor([[1, 2],\n        [3, 4]])\n\n\n\nnp_array = np.array(data)\nx_np = torch.from_numpy(np_array)\n\n\nx_ones = torch.ones_like(x_data) # retains the properties of x_data\nprint(f\"Ones Tensor: \\n {x_ones} \\n\")\n\nx_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\nprint(f\"Random Tensor: \\n {x_rand} \\n\")\n\nOnes Tensor: \n tensor([[1, 1],\n        [1, 1]]) \n\nRandom Tensor: \n tensor([[0.5371, 0.3625],\n        [0.6677, 0.8341]]) \n\n\n\n\nshape = (2,3,)\nrand_tensor = torch.rand(shape)\nones_tensor = torch.ones(shape)\nzeros_tensor = torch.zeros(shape)\n\nprint(f\"Random Tensor: \\n {rand_tensor} \\n\")\nprint(f\"Ones Tensor: \\n {ones_tensor} \\n\")\nprint(f\"Zeros Tensor: \\n {zeros_tensor}\")\n\nRandom Tensor: \n tensor([[0.4973, 0.1202, 0.9558],\n        [0.1171, 0.8799, 0.9128]]) \n\nOnes Tensor: \n tensor([[1., 1., 1.],\n        [1., 1., 1.]]) \n\nZeros Tensor: \n tensor([[0., 0., 0.],\n        [0., 0., 0.]])\n\n\n\ntensor = torch.rand(3,4)\n\nprint(f\"Shape of tensor: {tensor.shape}\")\nprint(f\"Datatype of tensor: {tensor.dtype}\")\nprint(f\"Device tensor is stored on: {tensor.device}\")\n\nShape of tensor: torch.Size([3, 4])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu\n\n\n\n# We move our tensor to the GPU if available\nif torch.cuda.is_available():\n  tensor = tensor.to('cuda')\n\n\ntensor = torch.ones(4, 4)\nprint('First row: ',tensor[0])\nprint('First column: ', tensor[:, 0])\nprint('Last column:', tensor[..., -1])\ntensor[:,1] = 0\nprint(tensor)\n\nFirst row:  tensor([1., 1., 1., 1.])\nFirst column:  tensor([1., 1., 1., 1.])\nLast column: tensor([1., 1., 1., 1.])\ntensor([[1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.]])\n\n\n\nt1 = torch.cat([tensor, tensor, tensor], dim=1)\nprint(t1)\n\ntensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n\n\n\n# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\ny1 = tensor @ tensor.T\ny2 = tensor.matmul(tensor.T)\n\ny3 = torch.rand_like(tensor)\ntorch.matmul(tensor, tensor.T, out=y3)\n\n\n# This computes the element-wise product. z1, z2, z3 will have the same value\nz1 = tensor * tensor\nz2 = tensor.mul(tensor)\n\nz3 = torch.rand_like(tensor)\ntorch.mul(tensor, tensor, out=z3)\n\ntensor([[1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.]])\n\n\n\nagg = tensor.sum()\nagg_item = agg.item()\nprint(agg_item, type(agg_item))\n\n12.0 <class 'float'>\n\n\n\nPytorch tensores are similar to numpy arrays as they are both specialized data structures that help us store and perfom operations like matrix multiplications on data more efficiently.\nTensors can run on GPUs or other hardware accelerators and they are also optimized for automatic differentiation (helps with math calculations of gradients, etc.)\nDeep learning can be very computationally expensive (more than ML), so by using tensors, we can speed up computations."
  },
  {
    "objectID": "posts/Auditing Allocative Bias/index.html",
    "href": "posts/Auditing Allocative Bias/index.html",
    "title": "Auditing Allocative Bias",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/index.html#goal",
    "href": "posts/Auditing Allocative Bias/index.html#goal",
    "title": "Auditing Allocative Bias",
    "section": "Goal",
    "text": "Goal\nCorrectly classify penguins by species (Gentoo, Chinstrap, Adelie)."
  },
  {
    "objectID": "posts/Auditing Allocative Bias/index.html#approach",
    "href": "posts/Auditing Allocative Bias/index.html#approach",
    "title": "Auditing Allocative Bias",
    "section": "Approach",
    "text": "Approach\n\nChoose Problem\n\nImport appropriate dataset for training\nChosse Features\n\nBasic Descriptives + visuals (if I’m not lazy)\nModel Training & Tuning\nModel Auditing\nConcluding Discussion"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/index.html#import-appropriate-dataset-for-training",
    "href": "posts/Auditing Allocative Bias/index.html#import-appropriate-dataset-for-training",
    "title": "Auditing Allocative Bias",
    "section": "Import appropriate dataset for training",
    "text": "Import appropriate dataset for training\n\n#talk a little about folktales\n\n\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"FL\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n  \n    \n      \n      RT\n      SERIALNO\n      DIVISION\n      SPORDER\n      PUMA\n      REGION\n      ST\n      ADJINC\n      PWGTP\n      AGEP\n      ...\n      PWGTP71\n      PWGTP72\n      PWGTP73\n      PWGTP74\n      PWGTP75\n      PWGTP76\n      PWGTP77\n      PWGTP78\n      PWGTP79\n      PWGTP80\n    \n  \n  \n    \n      0\n      P\n      2018GQ0000011\n      5\n      1\n      12702\n      3\n      12\n      1013097\n      9\n      64\n      ...\n      8\n      2\n      15\n      9\n      7\n      8\n      0\n      14\n      8\n      15\n    \n    \n      1\n      P\n      2018GQ0000056\n      5\n      1\n      9907\n      3\n      12\n      1013097\n      53\n      95\n      ...\n      53\n      52\n      53\n      4\n      98\n      3\n      52\n      102\n      100\n      102\n    \n    \n      2\n      P\n      2018GQ0000085\n      5\n      1\n      7102\n      3\n      12\n      1013097\n      26\n      15\n      ...\n      22\n      4\n      28\n      45\n      50\n      51\n      23\n      23\n      27\n      3\n    \n    \n      3\n      P\n      2018GQ0000092\n      5\n      1\n      3302\n      3\n      12\n      1013097\n      80\n      20\n      ...\n      79\n      136\n      145\n      12\n      84\n      12\n      80\n      79\n      14\n      74\n    \n    \n      4\n      P\n      2018GQ0000104\n      5\n      1\n      10501\n      3\n      12\n      1013097\n      31\n      18\n      ...\n      5\n      4\n      5\n      31\n      29\n      5\n      5\n      31\n      4\n      3\n    \n  \n\n5 rows × 286 columns"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/index.html#choose-features",
    "href": "posts/Auditing Allocative Bias/index.html#choose-features",
    "title": "Auditing Allocative Bias",
    "section": "Choose Features",
    "text": "Choose Features\n\npossible_features=['AGEP', 'COW', 'SCHL', 'MAR', 'OCCP', 'POBP', 'RELP', 'WKHP', 'RAC1P', 'SEX', 'PINCP'] #10 FEATURES FOR ACSIncome as listed in paper + PINCP as Target\nacs_data[possible_features].head()\n\n\n\n\n\n  \n    \n      \n      AGEP\n      COW\n      SCHL\n      MAR\n      OCCP\n      POBP\n      RELP\n      WKHP\n      RAC1P\n      SEX\n      PINCP\n    \n  \n  \n    \n      0\n      64\n      NaN\n      16.0\n      1\n      NaN\n      327\n      16\n      NaN\n      8\n      1\n      0.0\n    \n    \n      1\n      95\n      NaN\n      16.0\n      2\n      NaN\n      12\n      16\n      NaN\n      1\n      2\n      14500.0\n    \n    \n      2\n      15\n      NaN\n      12.0\n      5\n      NaN\n      12\n      16\n      NaN\n      2\n      1\n      0.0\n    \n    \n      3\n      20\n      1.0\n      16.0\n      5\n      5240.0\n      11\n      17\n      40.0\n      9\n      1\n      4900.0\n    \n    \n      4\n      18\n      NaN\n      16.0\n      5\n      NaN\n      40\n      17\n      NaN\n      2\n      1\n      0.0\n    \n  \n\n\n\n\n\nfeatures_to_use = [f for f in possible_features if f not in [\"PINCP\", \"SEX\"]]\n\n\nIncomeProblem = BasicProblem(\n    features=features_to_use,\n    target='PINCP',\n    target_transform=lambda x: x > 50000,\n    group='SEX',\n    preprocess=adult_filter,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\n\n\nfeatures, label, group = IncomeProblem.df_to_numpy(acs_data)\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/index.html#tuning-hyperparameters-with-cross-validation",
    "href": "posts/Auditing Allocative Bias/index.html#tuning-hyperparameters-with-cross-validation",
    "title": "Auditing Allocative Bias",
    "section": "Tuning hyperparameters with cross-validation",
    "text": "Tuning hyperparameters with cross-validation\n\n#do this loop with cross_validation\nfrom sklearn.model_selection import cross_val_score\n\ntrain_score_hist = []\ncrossval_score_hist = []\n\nfor i in range(1,12):\n    model = make_pipeline(StandardScaler(), RandomForestClassifier(max_depth=i, random_state=0))\n    model.fit(X_train, y_train)\n    \n    #cross-validation\n    model_scores = cross_val_score(model, X_train, y_train, cv=5)\n    crossval_score_hist.append(model_scores.mean())\n    \n\nnum_steps = len(crossval_score_hist)\nplt.plot(np.arange(num_steps) + 1, val_score_hist, label = \"val\")\n\nnum_steps = len(train_score_hist)\nplt.plot(np.arange(num_steps) + 1, train_score_hist, label = \"train\")\n\nValueError: x and y must have same first dimension, but have shapes (11,) and (24,)"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/index.html#overall-measures",
    "href": "posts/Auditing Allocative Bias/index.html#overall-measures",
    "title": "Auditing Allocative Bias",
    "section": "Overall Measures",
    "text": "Overall Measures\n\nPreliminaries\n\n#https://middlebury-csci-0451.github.io/CSCI-0451/lecture-notes/classification-in-practice.html for write-up explain\n\ny_hat_tests = model.predict(X_test) #gather predictions; but on test data thooo hmm suspect\n\nfrom sklearn.metrics import confusion_matrix\n\ndef PPV(cf):\n    \n    TP = cf[0,0]\n    FP = cf[0,1]\n    FN = cf[1,0]\n    TN = cf[1,1]\n    \n    return TP/(TP+FP)\n\ndef FPR(cf):\n    \n    TP = cf[0,0]\n    FP = cf[0,1]\n    FN = cf[1,0]\n    TN = cf[1,1]\n    \n    return FP/(FP+TN)\n\ndef FNR(cf):\n    \n    TP = cf[0,0]\n    FP = cf[0,1]\n    FN = cf[1,0]\n    TN = cf[1,1]\n    \n    return FP/(TP+FN)\n\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\ncm = confusion_matrix(y_test, y_hat_tests, labels=model.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=model.classes_)\ndisp.plot()\n\nplt.show()\n\n\n\n\n\n#Why do we do confusion matrix on test data? cuz this gives us meaningful results dummy\n\n\n\nOverall Measures\nThe model’s overall accuracy in predicting whether someone’s income is over $50,000 is:\n\noverall_accuracy = (y_hat == y_test).mean()\n\n0.8068233510235027\n\n\nThe model’s positive predictive value (PPV) is:\n\noverall_PPV = PPV(confusion_matrix(y_test, y_hat))\noverall_PPV\n\n0.880928965882442\n\n\nThe model’s overall false negative rate FNR is:\n\noverall_FNR = FNR(confusion_matrix(y_test, y_hat))\noverall_FNR\n\n0.11329446483840783\n\n\nThe model’s overall false negative rate FPR is:\n\noverall_FPR = FPR(confusion_matrix(y_test, y_hat))\noverall_FPR\n\n0.2671418873048201\n\n\n\n\nBy-Group Measures\nThe accuracy for male individuals is:\n\n(y_hat == y_test)[group_test == 1].mean()\n\n0.7906884987176958\n\n\n\nPPV(confusion_matrix(y_test[group_test == 1], y_hat[group_test == 1]))\n\n0.8671823568136933\n\n\n\nFNR(confusion_matrix(y_test[group_test == 1], y_hat[group_test == 1]))\n\n0.12256986634264884\n\n\n\nFPR(confusion_matrix(y_test[group_test == 1], y_hat[group_test == 1]))\n\n0.22706809229037703\n\n\nThe accuracy for female individuals is:\n\n(y_hat == y_test)[group_test == 2].mean()\n\n0.8237794132891054\n\n\n\nPPV(confusion_matrix(y_test[group_test == 2], y_hat[group_test == 2]))\n\n0.8926221475570488\n\n\nFNR(confusion_matrix(y_test[group_test == 2], y_hat[group_test == 2]))\n\nFPR(confusion_matrix(y_test[group_test == 2], y_hat[group_test == 2]))\n\n0.32805816937553467\n\n\n\n\nBias Measures\n\nCalibration\n\n(y_hat == y_test)[group_test == 1].mean()\n\n0.7906884987176958\n\n\n\n(y_hat == y_test)[group_test == 2].mean()\n\n0.8237794132891054\n\n\n\n\nError Rate Balance\n\n#fnr & fpr == for both; not the case\n\n\n\nStatistical Parity\nVery subjective picking what the worse classification is. In hands of gov. for taxes, maybe it’s worse to be predicted high, cuz you’ll pay more taxes and good for lower cuz you’ll get more benefits. Bad for people trying to charge you (loans maybe idk), cuz they’ll predict you have more money and charge you more.\n\n#do it for both\n\n\n(y_hat == 0)[group_test == 1].mean()\n\n0.64943775892681\n\n\n\n(y_hat == 0)[group_test == 2].mean()\n\n0.7576448636881932\n\n\n\n(y_hat == 1)[group_test == 1].mean()\n\n0.35056224107318995\n\n\n\n(y_hat == 1)[group_test == 2].mean()\n\n0.24235513631180677"
  },
  {
    "objectID": "posts/Linear Regression/index.html#least-squares-linear-regression",
    "href": "posts/Linear Regression/index.html#least-squares-linear-regression",
    "title": "Linear Regression",
    "section": "Least-Squares Linear Regression",
    "text": "Least-Squares Linear Regression\nSince we have a linear model, our predictions will be linear: \\(\\hat{y}_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\)\nThe loss function we will use for our implementation of linear regression is the squared loss: \\(\\ell(\\hat{y}, y) = (\\hat{y} - y)^2\\)\nTo obtain our desired parameter vector, we want to minimize the emprirical risk of our training model as shown below:\n\\[\\begin{aligned}\n\\hat{\\mathbf{w}} &= \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{w}} \\; L(\\mathbf{w}) \\\\\n          &= \\sum_{i = 1}^n \\ell(\\hat{y}_i, y_i) \\\\\n          &= \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{w}} \\sum_{i = 1}^n \\left(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle - y_i \\right)^2\\;.\n\\end{aligned}\\]\nThe equation takes the following form in matrix-vector notation:\n\\[\\begin{aligned}\n\\hat{\\mathbf{w}} = \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{w}} \\; L(\\mathbf{w}) = \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{w}} \\; \\lVert \\mathbf{X}\\mathbf{w}- \\mathbf{y} \\rVert_2^2\\;.\n\\end{aligned}\\]\nSince squared loss is a convex function, we are guaranteed to find any local minimum, which will be the global minimum by the definition of convexity.\nBy the definition of convexity, the minimum is located at the point where the empirical risk function’s gradient is 0.\n\nAnalytic Approach\nIf we set the\n\n\nGradient Descent Approach\nGradient is descent direction. Every iteration we calculate it and move in the descen\n\n#This function helps modify our given feature array by adding column of 1's \ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\n#This function will create both testing and validation data \ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = 1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\n#choose # of samples and features\nn_train = 100\nn_val = 100\np_features = 1\n\n#adjusts variability of the data\nnoise = 0.1\n\n#create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n\nweight = 0\nscore_history = []\n\nlr = LinearRegression(weight, score_history)\n\nlr.fit_gradient(X_train, y_train, alpha = 0.095, max_epochs = 2000)\nprint(lr.w)\n\nlr.fit_analytic(X_train, y_train)\nprint(lr.w)\n\n[0.78541939 0.84879354]\n[0.78541939 0.84879354]\n\n\nSame for both fit methods yayy\n\nprint(f\"Training score = {lr.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {lr.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.858\nValidation score = 0.8287\n\n\n\n# plot it\n\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\n\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter((X_val), y_val)\n\n\n#pad for graphing the line hmmmm!!!; messes up the fit\n\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\naxarr[0].plot(pad(X_train)[:,0], pad(X_train)@lr.w, color = \"black\") #[:,0]gives 1st column vs. print(X_train[0]) #gives 1st enxtry\naxarr[1].plot(pad(X_val)[:,0], pad(X_val)@lr.w, color = \"black\") \n\n\nlabs = axarr[0].set(title = \"Training\", xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\nplt.tight_layout()\n\n\n#fake data set 0,1 (101 points)\n#linspace [newaxis]\n#pad it\n#use it to plot\n\n\n\n\n\n#comparison of analy. and gradient; just print out and comment on it\n\nModel does a good job on both the training and validation data.\n\nplt.plot(lr.score_history)\nplt.ylim([0, 1])\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\n\n# comment on this score and its format"
  },
  {
    "objectID": "posts/Linear Regression/index.html#lasso",
    "href": "posts/Linear Regression/index.html#lasso",
    "title": "Linear Regression",
    "section": "Lasso",
    "text": "Lasso\n\n#debrief\n\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.001)\n\nlasso_score_hist = []\n\np_features = n_train - 1\n\n\n#i is the p_features\nfor i in range(1, (n_train-1)+20):\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, i, noise) #behaving more normally\n    L.fit(X_train, y_train)\n    lasso_score_hist.append((L.score(X_val, y_val)))\n\nnum_steps = len(lasso_score_hist)\n\n#graph this beauty\nplt.plot(np.arange(num_steps) + 1, lasso_score_hist, label = \"lasso\")\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n/Users/hedavamsolano/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.488e-02, tolerance: 4.877e-02\n  model = cd_fast.enet_coordinate_descent(\n/Users/hedavamsolano/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.536e-01, tolerance: 6.615e-02\n  model = cd_fast.enet_coordinate_descent(\n\n\n\n\n\n\n#comment"
  },
  {
    "objectID": "Untitled.html",
    "href": "Untitled.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "china.shape\n\n(427, 640, 3)\n\n\n\ndata = china / 255.0 # use 0...1 scale\ndata = data.reshape(427 * 640, 3)\ndata.shape\n\n(273280, 3)\n\n\n\ndef plot_pixels(data, title, colors=None, N=10000):\n    if colors is None:\n        colors = data\n    \n    # choose a random subset\n    rng = np.random.RandomState(0)\n    i = rng.permutation(data.shape[0])[:N]\n    colors = colors[i]\n    R, G, B = data[i].T\n    \n    fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n    ax[0].scatter(R, G, color=colors, marker='.')\n    ax[0].set(xlabel='Red', ylabel='Green', xlim=(0, 1), ylim=(0, 1))\n\n    ax[1].scatter(R, B, color=colors, marker='.')\n    ax[1].set(xlabel='Red', ylabel='Blue', xlim=(0, 1), ylim=(0, 1))\n\n    fig.suptitle(title, size=20);\n\n\nplot_pixels(data, title='Input color space: 16 million possible colors')\n\n\n\n\n\nimport warnings; warnings.simplefilter('ignore')  # Fix NumPy issues.\n\nfrom sklearn.cluster import MiniBatchKMeans\nkmeans = MiniBatchKMeans(32)\nkmeans.fit(data)\nnew_colors = kmeans.cluster_centers_[kmeans.predict(data)]\n\nplot_pixels(data, colors=new_colors,\n            title=\"Reduced color space: 16 colors\")\n\n\n\n\n\nchina_recolored = new_colors.reshape(china.shape)\n\nfig, ax = plt.subplots(1, 2, figsize=(16, 6),\n                       subplot_kw=dict(xticks=[], yticks=[]))\nfig.subplots_adjust(wspace=0.05)\nax[0].imshow(china)\nax[0].set_title('Original Image', size=16)\nax[1].imshow(china_recolored)\nax[1].set_title('16-color Image', size=16);\n\n\n\n\n\n4/12/23 Warmup\n\nfrom PIL import Image\nimport urllib\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef read_image(url):\n    return np.array(Image.open(urllib.request.urlopen(url)))\n\nurl = \"https://i.pinimg.com/originals/0e/d0/23/0ed023847cad0d652d6371c3e53d1482.png\"\n\nimg = read_image(url)\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\nimg = to_greyscale(img)\n\nplt.imshow(img, cmap = \"Greys\")\nplt.gca().axis(\"off\")\n\narray([[-251.9747, -250.9748, -251.9747, ..., -251.9747, -251.9747,\n        -251.9747],\n       [-251.9747, -250.9748, -250.9748, ..., -250.9748, -251.9747,\n        -251.9747],\n       [-251.9747, -251.9747, -251.9747, ..., -251.9747, -251.9747,\n        -251.9747],\n       ...,\n       [-253.9745, -253.9745, -253.9745, ..., -253.9745, -252.9746,\n        -253.9745],\n       [-252.9746, -253.9745, -252.9746, ..., -253.9745, -252.9746,\n        -253.9745],\n       [-253.9745, -252.9746, -252.9746, ..., -253.9745, -253.9745,\n        -253.9745]])\n\n\n\n\n\n\nimg.shape\n\n(413, 640)\n\n\n\n411/3\n\n137.0\n\n\n\n(img[0:0+3,0:0+3]@kernel).sum()\n\n8.999100000000226\n\n\n\nnp.sum(img[0:3, 0:3]*kernel)\n\n5.999400000000065\n\n\n\n#edge detection algo\ndef myconvolve2d(img, kernel):\n    output = np.zeros((img.shape[0], img.shape[1])) #valid padding, I think\n    \n    output = np.pad\n    \n    #do better padding and I won't need -2 in the loops\n    #need \n    #loop through pixels \n    for i in range(img.shape[0]-2): #height\n        for j in range(img.shape[1]-2): #witdh\n            output[i][j] = (img[i:i+3,j:j+3]*kernel).sum() #[i:i+3,j:j+3] grabs 3 x 3 subset of our img matrix #multiply and add; #dot product is not the same as matrix multiplication dummy\n\n    return output\n\nmyconvolve2d(img, kernel)\n\n\narray([[ 5.9994,  4.9995,  5.9994, ..., -0.9999,  0.    ,  0.    ],\n       [-3.9996, -3.9996, -3.9996, ..., -1.9998,  0.    ,  0.    ],\n       [-2.9997,  6.9993, -2.9997, ..., -0.9999,  0.    ,  0.    ],\n       ...,\n       [-3.9996,  5.9994, -3.9996, ...,  6.9993,  0.    ,  0.    ],\n       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]])\n\n\n\nfrom scipy.signal import convolve2d\n\nkernel = np.array([[-1, -1, -1], \n                   [-1,  8, -1], \n                   [-1, -1, -1]])\n\nconvd = myconvolve2d(img,kernel)\n\n\n\n\nplt.imshow(convd, cmap = \"Greys\", vmin = 0, vmax = 8)\nplt.gca().axis(\"off\")\n\n(-0.5, 639.5, 412.5, -0.5)\n\n\n\n\n\n\ndef convolution2d(image, kernel):\n    m, n = kernel.shape\n    if (m == n):\n        y, x = image.shape\n        y = y - m + 1\n        x = x - m + 1\n        print(y)\n        print(x)\n        new_image = np.zeros((y,x))\n        print(new_image.shape)\n        for i in range(y):\n            for j in range(x):\n                new_image[i][j] = np.sum(image[i:i+m, j:j+m]*kernel) #\n    return new_image\n\n\n\nLinear Regression Continued\n\n#important features (ask why positive w means features are important)??\n\n\n#positive means that there's a high correlation(?) between this feature and the ridership prediction\n\n#like it makes the prediction be higher (duh)\n\n\n\nsorted_ascend_params = np.argsort(lr_bike.w) #Returns the indices that would sort an array in ascending order\n\n\n#top 5 most \"useful\" features\n# for i in range(1,5):\n#     print(X_train_bike.columns[sorted_ascend_params[-i:]])\n\nX_train_bike.columns[sorted_ascend_params[i]]\n\nIndexError: index 18 is out of bounds for axis 0 with size 18\n\n\n\n(X_train_bike.columns).shape[0]\n\n18\n\n\n\n(lr_bike.w).shape[0] #take out last column because it's the bias? yea\n\n19\n\n\n\nprint(sorted_ascend_params)\n\n[ 5  1  4  6  0  7 17 12 16 13  2  8 11 14 15  9 10 18  3]\n\n\n\nprint((X_train_bike.columns))\nprint((lr_bike.w))\n\nIndex(['weathersit', 'workingday', 'yr', 'temp', 'hum', 'windspeed', 'holiday',\n       'mnth_2', 'mnth_3', 'mnth_4', 'mnth_5', 'mnth_6', 'mnth_7', 'mnth_8',\n       'mnth_9', 'mnth_10', 'mnth_11', 'mnth_12'],\n      dtype='object')\n[-1.25191790e+02 -7.89386722e+02  2.85816915e+02  1.45898411e+03\n -3.76119347e+02 -9.59511991e+02 -2.34118970e+02  2.84441373e-01\n  3.73559040e+02  5.21961581e+02  5.48801456e+02  3.85722538e+02\n  2.59139107e+02  2.61659311e+02  3.89647869e+02  4.48576031e+02\n  2.61221557e+02  1.03399391e+02  8.19990202e+02]"
  }
]