[
  {
    "objectID": "posts/Optimization for Logistic Regression/index.html",
    "href": "posts/Optimization for Logistic Regression/index.html",
    "title": "Optimization for Logistic Regression Blog",
    "section": "",
    "text": "The implementation of the perceptron algorithm could be found here: https://github.com/Hedavam/Hedavam.github.io/blob/main/posts/Optimization%20for%20Logistic%20Regression/lr.py\nThis article was helpful for drafting this blog post: https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a"
  },
  {
    "objectID": "posts/Optimization for Logistic Regression/index.html#batch",
    "href": "posts/Optimization for Logistic Regression/index.html#batch",
    "title": "Optimization for Logistic Regression Blog",
    "section": "Batch",
    "text": "Batch\nBatch gradient descent makes use of all the training data (every epoch), computes the gradient for each sample and uses the average of these gradients for the actual gradient step as it subtracts this average gradient from the parameters for however many epochs (iterations) are needed for convergence or however many epochs the user specifies as the max (hyperparameter). There’s also another hyperparameter, the learning rate, which adjusts our step size. Mathematically, if our step size is small enough, this algorithm will converge."
  },
  {
    "objectID": "posts/Optimization for Logistic Regression/index.html#stochastic",
    "href": "posts/Optimization for Logistic Regression/index.html#stochastic",
    "title": "Optimization for Logistic Regression Blog",
    "section": "Stochastic",
    "text": "Stochastic\nStochastic gradient descent also makes use of all the training data (every epoch), but divides it into batches to compute gradients and take steps with this gradient for this batches. This process is repeated for the remaining batches of the training data. The hypermarameter in this algorithm is the size of the batches.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n\nfrom sklearn.datasets import make_blobs\nfrom lr import LogisticRegression"
  },
  {
    "objectID": "posts/Optimization for Logistic Regression/index.html#speed-comparison-for-batch-vs.-stochastic-gradient-descent",
    "href": "posts/Optimization for Logistic Regression/index.html#speed-comparison-for-batch-vs.-stochastic-gradient-descent",
    "title": "Optimization for Logistic Regression Blog",
    "section": "Speed comparison for batch vs. stochastic gradient descent",
    "text": "Speed comparison for batch vs. stochastic gradient descent\n\nLR = LogisticRegression(weight, empirical_risk_history, score_hist)\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10)  #small batch size converges faster; does a lot more updates (in inner loop)?\n\nnum_steps = len(LR.loss_history)\nprint(num_steps)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression(weight, empirical_risk_history, score_hist)\nLR.fit(X, y, alpha = .01, max_epochs = 1000) #.005 and 10000 converges nearly every time; .01 and 1000 converges; #.01 is too big for 1000 epochs\n\nnum_steps = len(LR.loss_history)\nprint(num_steps)\n\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\nConverged\n12\nConveged\n570\n\n\n\n\n\nVisualization of the number of iterations it took linear regression algorithm to converge for batch vs. stochastic gradient descent\n\nLR = LogisticRegression(weight, empirical_risk_history, score_hist)\nLR.fit_stochastic(X, y, alpha = 0.1, max_epochs = 1000, batch_size = 10)  #small batch size converges faster; does a lot more updates (in inner loop)?\nnum_steps = len(LR.loss_history)\nprint(num_steps)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\n\nLR = LogisticRegression(weight, empirical_risk_history, score_hist)\nLR.fit(X, y, alpha = .01, max_epochs = 1000) #.005 and 10000 converges nearly every time; .01 and 1000 converges; #.01 is too big for 1000 epochs\nnum_steps = len(LR.loss_history)\nprint(num_steps)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient (small lr)\")\n\n\nLR = LogisticRegression(weight, empirical_risk_history, score_hist)\nLR.fit(X, y, alpha = .5, max_epochs = 1000) #.005 and 10000 converges nearly every time; .01 and 1000 converges; #.01 is too big for 1000 epochs\nnum_steps = len(LR.loss_history)\nprint(num_steps)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient (big lr)\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\nConverged\n3\nConveged\n542\n1000\n\n\n\n\n\n\nTake-away 1: Stochastic gradient descent converges faster than batch gradient descent because it updates the parameters more frequently.\nFor stochastic gradient descent, in one epoch, we take many gradient steps (one for each batch) while for batch gradient descent we only take one gradient step for every epoch.\n\n\nTake-away 2: Too large of a learning rate will prevent linear regresion model optimized with gradient descent from converging."
  },
  {
    "objectID": "posts/Auditing Allocative Bias/index.html",
    "href": "posts/Auditing Allocative Bias/index.html",
    "title": "Auditing Allocative Bias",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/index.html#goal",
    "href": "posts/Auditing Allocative Bias/index.html#goal",
    "title": "Auditing Allocative Bias",
    "section": "Goal",
    "text": "Goal\nCreat a machine learning model that predicts if someone’s income is over $50K on the basis of sex. Then, we’ll perform a fairness audit to assess if our algorithm has a gender bias."
  },
  {
    "objectID": "posts/Auditing Allocative Bias/index.html#approach",
    "href": "posts/Auditing Allocative Bias/index.html#approach",
    "title": "Auditing Allocative Bias",
    "section": "Approach",
    "text": "Approach\n\nChoose Problem\n\nImport appropriate dataset for training\nChosse Features\n\nBasic Descriptives + visuals (if I’m not lazy)\nModel Training & Tuning\nModel Auditing\nConcluding Discussion\n\nUseful Resources: - https://middlebury-csci-0451.github.io/CSCI-0451/lecture-notes/classification-in-practice.html"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/index.html#import-appropriate-dataset-for-training",
    "href": "posts/Auditing Allocative Bias/index.html#import-appropriate-dataset-for-training",
    "title": "Auditing Allocative Bias",
    "section": "Import appropriate dataset for training",
    "text": "Import appropriate dataset for training\n\nfrom folktables import ACSDataSource, ACSEmploymalet, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"FL\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n  \n    \n      \n      RT\n      SERIALNO\n      DIVISION\n      SPORDER\n      PUMA\n      REGION\n      ST\n      ADJINC\n      PWGTP\n      AGEP\n      ...\n      PWGTP71\n      PWGTP72\n      PWGTP73\n      PWGTP74\n      PWGTP75\n      PWGTP76\n      PWGTP77\n      PWGTP78\n      PWGTP79\n      PWGTP80\n    \n  \n  \n    \n      0\n      P\n      2018GQ0000011\n      5\n      1\n      12702\n      3\n      12\n      1013097\n      9\n      64\n      ...\n      8\n      2\n      15\n      9\n      7\n      8\n      0\n      14\n      8\n      15\n    \n    \n      1\n      P\n      2018GQ0000056\n      5\n      1\n      9907\n      3\n      12\n      1013097\n      53\n      95\n      ...\n      53\n      52\n      53\n      4\n      98\n      3\n      52\n      102\n      100\n      102\n    \n    \n      2\n      P\n      2018GQ0000085\n      5\n      1\n      7102\n      3\n      12\n      1013097\n      26\n      15\n      ...\n      22\n      4\n      28\n      45\n      50\n      51\n      23\n      23\n      27\n      3\n    \n    \n      3\n      P\n      2018GQ0000092\n      5\n      1\n      3302\n      3\n      12\n      1013097\n      80\n      20\n      ...\n      79\n      136\n      145\n      12\n      84\n      12\n      80\n      79\n      14\n      74\n    \n    \n      4\n      P\n      2018GQ0000104\n      5\n      1\n      10501\n      3\n      12\n      1013097\n      31\n      18\n      ...\n      5\n      4\n      5\n      31\n      29\n      5\n      5\n      31\n      4\n      3\n    \n  \n\n5 rows × 286 columns"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/index.html#choose-features",
    "href": "posts/Auditing Allocative Bias/index.html#choose-features",
    "title": "Auditing Allocative Bias",
    "section": "Choose Features",
    "text": "Choose Features\n\npossible_features=['AGEP', 'COW', 'SCHL', 'MAR', 'OCCP', 'POBP', 'RELP', 'WKHP', 'RAC1P', 'SEX', 'PINCP'] #10 FEATURES FOR ACSIncome as listed in paper + PINCP as Target\nacs_data[possible_features].head()\n\n\n\n\n\n  \n    \n      \n      AGEP\n      COW\n      SCHL\n      MAR\n      OCCP\n      POBP\n      RELP\n      WKHP\n      RAC1P\n      SEX\n      PINCP\n    \n  \n  \n    \n      0\n      64\n      NaN\n      16.0\n      1\n      NaN\n      327\n      16\n      NaN\n      8\n      1\n      0.0\n    \n    \n      1\n      95\n      NaN\n      16.0\n      2\n      NaN\n      12\n      16\n      NaN\n      1\n      2\n      14500.0\n    \n    \n      2\n      15\n      NaN\n      12.0\n      5\n      NaN\n      12\n      16\n      NaN\n      2\n      1\n      0.0\n    \n    \n      3\n      20\n      1.0\n      16.0\n      5\n      5240.0\n      11\n      17\n      40.0\n      9\n      1\n      4900.0\n    \n    \n      4\n      18\n      NaN\n      16.0\n      5\n      NaN\n      40\n      17\n      NaN\n      2\n      1\n      0.0\n    \n  \n\n\n\n\n\n#take out sex (feature we're auditing bias for) and total person’s income (target)\nfeatures_to_use = [f for f in possible_features if f not in [\"PINCP\", \"SEX\"]]\n\n\nIncomeProblem = BasicProblem(\n    features=features_to_use,\n    target='PINCP',\n    target_transform=lambda x: x > 50000, #0 if PINCP <=50k; #1 if PINCP >50k\n    group='SEX',\n    preprocess=adult_filter,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = IncomeProblem.df_to_numpy(acs_data)\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/index.html#overall-measures",
    "href": "posts/Auditing Allocative Bias/index.html#overall-measures",
    "title": "Auditing Allocative Bias",
    "section": "Overall Measures",
    "text": "Overall Measures\n\nThe model’s overall accuracy in predicting whether someone’s income is over $50,000 is:\n\n(y_hat_tests == y_test).mean()\n\n0.8022744503411675\n\n\n\n\nThe model’s overall positive predictive value (PPV) is:\n\nbias_rates(confusion_matrix(y_test, y_hat_tests), \"PPV\")\n\n0.7387549478229579\n\n\n\n\nThe model’s overall false negative rate FNR is:\n\nbias_rates(confusion_matrix(y_test, y_hat_tests), \"FNR\")\n\n0.37465732561681386\n\n\n\n\nThe model’s overall false positive rate FPR is:\n\nbias_rates(confusion_matrix(y_test, y_hat_tests), \"FPR\")\n\n0.10984189424313488\n\n\n\n\nBy-Group Measures\n\n\nFor Male Individuals\n\nThe accuracy is:\n\n(y_hat_tests == y_test)[group_test == 1].mean()\n\n0.7819096468731506\n\n\n\n\nThe model’s positive predictive value (PPV) is:\n\nbias_rates(confusion_matrix(y_test[group_test == 1], y_hat_tests[group_test == 1]), \"PPV\")\n\n0.7822506861848124\n\n\n\n\nThe model’s false negative rate FNR is:\n\nbias_rates(confusion_matrix(y_test[group_test == 1], y_hat_tests[group_test == 1]), \"FNR\")\n\n0.3685376661742984\n\n\n\n\nThe model’s false positive rate FPR is:\n\nbias_rates(confusion_matrix(y_test[group_test == 1], y_hat_tests[group_test == 1]), \"FPR\")\n\n0.1175115207373272\n\n\n\n\n\nFor Female Individuals\n\nThe accuracy for female individuals is:\n\n(y_hat_tests == y_test)[group_test == 2].mean()\n\n0.8236757541204519\n\n\n\n\nThe model’s positive predictive value (PPV) is:\n\nbias_rates(confusion_matrix(y_test[group_test == 2], y_hat_tests[group_test == 2]), \"PPV\")\n\n0.6761737604212373\n\n\n\n\nThe model’s false negative rate FNR is:\n\nbias_rates(confusion_matrix(y_test[group_test == 2], y_hat_tests[group_test == 2]), \"FNR\")\n\n0.3845846645367412\n\n\n\n\nThe model’s false positive rate FPR is:\n\nbias_rates(confusion_matrix(y_test[group_test == 2], y_hat_tests[group_test == 2]), \"FPR\")\n\n0.10331793364132717\n\n\nFor our binary sex-based income classification task, our overall measures tell us:\n\nThe model has a slightly higher accuracy for females than for males (about a 4% difference)\n\nAccuracy is the # of correct predictions / # of total predictions\nBasically, the likelihood that the model’s prediction matches the actual label.\n\nThe model’s PPV is significantly higher for males than for females (about a 10 % difference), meaning that when the model predicts that males make >\\$50k, it is right 10% more often than when it predicts that females make >\\$50k.\n\nPPV is the likelihood that for a given positive prediction, the corresponding actual label that was also positive.\n\nThe model has very similar FNR and FPR rates for both groups, meaning that the rate at which the model fail to identify individuals making >\\$50k and <\\$50k is almost the same for both groups.\n\nFNR is the the likelihood that for a given actual label that is a positive, the model incorectly predicts a negative label. Basically, failure to identify positive cases.\nFPR is the the likelihood that for a given actual label that is a negative, the model incorectly predicts a positive label. Basically, failure to identify negative cases.\n\n\nNote: “Positive” and “negative” are not indicative good or bad. In this context, “positive” means making >$50k, “negative” means making <\\$50k."
  },
  {
    "objectID": "posts/Auditing Allocative Bias/index.html#bias-measures",
    "href": "posts/Auditing Allocative Bias/index.html#bias-measures",
    "title": "Auditing Allocative Bias",
    "section": "Bias Measures",
    "text": "Bias Measures\n\nCalibration\nAs defined by Chouldechova in this paper, “a score S = S(x) is said to be well-calibrated if it reflects the same likelihood of recidivism irrespective of the individuals’ group membership.”\nIn our context, when we are doing binary classification for sex-based income, the calibration takes on the form of the PPV. Namely, our model would be well-calibrated if the fraction of predicted individuals making >\\$50k who actually make >\\$50k is the same across groups.\n\ndf2 = pd.DataFrame(X_test, columns = features_to_use) \n\n\ndf2[\"group\"] = group_test\ndf2[\"label\"] = y_test \ndf2[\"predicted_label\"] = y_hat_tests\n\n\ndf2_calibration = df2.query(\"predicted_label == True\")\n\n\ndf2_calibration = df2_calibration.groupby([\"group\", \"predicted_label\"])[\"label\"].mean().reset_index(name = \"mean\")\n\n\nsns.set_theme(style=\"whitegrid\")\n\np = sns.catplot(data = df2_calibration, kind = \"bar\", x = \"predicted_label\", y = \"mean\", hue = \"group\", legend_out = False)\nnew_labels = ['Male', 'Female']\nfor t, l in zip(p._legend.texts, new_labels):\n    t.set_text(l)\n\nsns.move_legend(p, \"upper right\")\n\n\n\n\nOf those who were predicted to make >\\$50k, more males actually make >\\$50k.\n\n\nError Rate Balance\nA prediction satisfies error rate balance if the FPR and FNR are equal across groups.\n\nfig, axarr = plt.subplots(1, 3, figsize=(15,3.2), sharex = False, sharey = False)\n\n#overall\ncm1 = confusion_matrix(y_test, y_hat_tests, labels=model.classes_, normalize = \"true\")\ndisp1 = ConfusionMatrixDisplay(confusion_matrix=cm1,\n                              display_labels=model.classes_)\n\n#male\ncm2 = confusion_matrix(y_test[group_test == 1], y_hat_tests[group_test == 1], labels=model.classes_)\ndisp2 = ConfusionMatrixDisplay(confusion_matrix=cm2,\n                              display_labels=model.classes_)\n\n#female\ncm3 = confusion_matrix(y_test[group_test == 2], y_hat_tests[group_test == 2], labels=model.classes_)\ndisp3 = ConfusionMatrixDisplay(confusion_matrix=cm3,\n                              display_labels=model.classes_)\n\ndisp1.plot(ax=axarr[0], cmap = plt.cm.Blues)\naxarr[0].grid(visible=None)\n\ndisp2.plot(ax=axarr[1], cmap = plt.cm.Blues)\naxarr[1].grid(visible=None)\n\ndisp3.plot(ax=axarr[2], cmap = plt.cm.Blues)\naxarr[2].grid(visible=None)\n\n\n\n\n\nAs observed in the overall measures section, the FPR and FNR are nearly the same across groups.\n\n\nStatistical Parity\nA prediction satisfies statistical parity if the likelihood of getting a certain prediction is the same across groups.\nVery subjective picking what the worse classification is. In hands of gov. for taxes, maybe it’s worse to be predicted high, cuz you’ll pay more taxes and good for lower cuz you’ll get more benefits. Bad for people trying to charge you (loans maybe idk), cuz they’ll predict you have more money and charge you more.\n\ndf2_stat_parity_v = df2.groupby([\"group\"], as_index=False)[\"predicted_label\"].value_counts(normalize = True)\n\n\ndf2_stat_parity_v\n\n\n\n\n\n  \n    \n      \n      group\n      predicted_label\n      proportion\n    \n  \n  \n    \n      0\n      1\n      False\n      0.676563\n    \n    \n      1\n      1\n      True\n      0.323437\n    \n    \n      2\n      2\n      False\n      0.763761\n    \n    \n      3\n      2\n      True\n      0.236239\n    \n  \n\n\n\n\n\nsns.set_theme(style=\"whitegrid\")\n\np = sns.catplot(data = df2_stat_parity_v, kind = \"bar\", x = \"predicted_label\", y = \"proportion\", hue = \"group\", legend_out = False)\nnew_labels = ['Male', 'Female']\nfor t, l in zip(p._legend.texts, new_labels):\n    t.set_text(l)\n\nsns.move_legend(p, \"upper right\")\n\n\n\n\nThe model is more likely to predict that males make >50k and that females make <\\$50k."
  },
  {
    "objectID": "posts/Learning from Timnit Gebru/index.html",
    "href": "posts/Learning from Timnit Gebru/index.html",
    "title": "Learning from Timnit Gebru",
    "section": "",
    "text": "Part 2: Notes on the Talk\nDr. Gebru began her argument by giving a brief introduction of AGI and how some people believe it will be used to achieve an utopian society.\nIn the first section of the talk, Dr. Gebru focused on discussing second-wave eugenics and more specifically the properties of the TESCREAL bundle (transhumanism, extropianism, singularitarianism, cosmism, Rationalism, effective altruism, and longtermism). She went through each term, providing definitions for the specific ideology, naming people associated with it, and laid out the ties between AGI and these ideologies. A memorable ideology of the TESCREAL was cosmism. Dr. Gebru pointed out how Goertzel, one of the researchers that “christened” the term AGI believe that “morally superior beings produced by AGI may advance the good of the cosmos” (@timnitGebru on Twitter). I think what Dr. Gebru wanted us to take-away from this section of the talk is that the ideologies of the TESCREAL bundle are rooted in eugenics and are a negative thing to advance by developing AGI.\nThe next section of the talk focused on the two extreme eschatalogical implications of AGI: utopia and apocalypse. An AGI utopia as envisioned by some consists of AGI being able to find out the “best” thing to do in any scenario and ideally helping us achieve world peace. Others think an AGI utopia will be achieved through “morally superior” AGI enhanced transhuman minds that will benefit our society. Dr. Gebru urged us to question who an AGI utopia would be benefitting - claiming that it wouldn’t benefit those in marginalized communities, but would instead benefit those in dominant positions of power. Regarding the AGI apocalypse, Dr. Gebru emphasized the importance of acknowdledging the systems driving the “machines” down dangerous paths and the negative impacts of their decisions. Namely, big corporations and the military funding research in these areas, which result in the environment being harmed, datasets being stolen, and workers being exploited.\nHer preferred alternative to AGI seemed to be having subject area experts curating datasets and working on smaller models with a specific use case as she pointed out in her example of the language translation some of her colleagues worked on.\nMy perspective: I’m marveled by AGI tools like ChatGPT and think that they are more useful than they are harmful, for right now. However, I share the same worries many people do when it comes to the advancement in this technology as I do not believe that equity will be at its forefront. Like we’ve learned in ML, there’s always a trade-off. These tools are and will likely remain for-profit, meaning there has to be a loss somewhere. I fear that those who lose will be those who have been losing throughout the course of history because of oppressive systems. Sure, AGI doesn’t require much labor as the goal is for it to learn on its own, but to develop AGI powerful enough to stand on its own, many people will disadvantaged. Also, it’s scary to think about the job market if AGI is succesfully develop. It’s hard to imagine a world where most jobs are done by machines. Production might be increased, but perhaps at the expense of quality. If AGI becomes mainstream, will the majority of jobs available be in the CS field? A whole society of computer scientists doesn’t sound like the best thing ever. Creativity surely would decline. I’m getting way ahead of myself as I don’t think (or at least hope) we’re anywhere close to experiencing these huge shifts caused by AGI in our society, but these are just the thoughts that run through my mind when thinking of how far left these things can go.\n\n\nPart 3: Reflection on the Process\nI appreciated that Dr. Gebru asks us to look at AI with a critical eye. I was unaware of how unethical data collection and content moderation practices could be and will be more keen to things like that in the future. With these ethical concerns in mind, I hope to be able to contribute to building more ethical AI. I felt empowered by Dr. Gebru’s take on impostor syndrome as she said been in an environment conducive to shared progress helps you feel less like an impostor. She also reminded us that there are many systems in place that make you feel like an impostor, when, in fact, you are not.\nSomething about the process I found discouraging was the extreme beliefs held by people with influence in the AI field. I doubt (and pray) that there will not be an AGI apocalypse of AGI utopia, but many people feel very strongly about either being the case. The animosity that ensued from the first question posed after the talk made me feel uncomfortable and I think is cause for concern as the rise of AGI will surely propogate more moments like that (and perhaps some with more serious consequences for others)."
  },
  {
    "objectID": "posts/Classifying Palmer Penguins - ML Workflow/index.html",
    "href": "posts/Classifying Palmer Penguins - ML Workflow/index.html",
    "title": "My CSCI 0451 Blog",
    "section": "",
    "text": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\ntrain.head(10)\n\n\n\n\n\n  \n    \n      \n      studyName\n      Sample Number\n      Species\n      Region\n      Island\n      Stage\n      Individual ID\n      Clutch Completion\n      Date Egg\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Sex\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Comments\n    \n  \n  \n    \n      0\n      PAL0708\n      27\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N46A1\n      Yes\n      11/29/07\n      44.5\n      14.3\n      216.0\n      4100.0\n      NaN\n      7.96621\n      -25.69327\n      NaN\n    \n    \n      1\n      PAL0708\n      22\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N41A2\n      Yes\n      11/27/07\n      45.1\n      14.5\n      215.0\n      5000.0\n      FEMALE\n      7.63220\n      -25.46569\n      NaN\n    \n    \n      2\n      PAL0910\n      124\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N67A2\n      Yes\n      11/16/09\n      41.4\n      18.5\n      202.0\n      3875.0\n      MALE\n      9.59462\n      -25.42621\n      NaN\n    \n    \n      3\n      PAL0910\n      146\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N82A2\n      Yes\n      11/16/09\n      39.0\n      18.7\n      185.0\n      3650.0\n      MALE\n      9.22033\n      -26.03442\n      NaN\n    \n    \n      4\n      PAL0708\n      24\n      Chinstrap penguin (Pygoscelis antarctica)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N85A2\n      No\n      11/28/07\n      50.6\n      19.4\n      193.0\n      3800.0\n      MALE\n      9.28153\n      -24.97134\n      NaN\n    \n    \n      5\n      PAL0809\n      99\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N50A1\n      Yes\n      11/10/08\n      33.1\n      16.1\n      178.0\n      2900.0\n      FEMALE\n      9.04218\n      -26.15775\n      NaN\n    \n    \n      6\n      PAL0708\n      17\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N9A1\n      Yes\n      11/12/07\n      38.7\n      19.0\n      195.0\n      3450.0\n      FEMALE\n      9.18528\n      -25.06691\n      NaN\n    \n    \n      7\n      PAL0910\n      131\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Torgersen\n      Adult, 1 Egg Stage\n      N73A1\n      No\n      11/23/09\n      38.5\n      17.9\n      190.0\n      3325.0\n      FEMALE\n      8.98460\n      -25.57956\n      Nest never observed with full clutch.\n    \n    \n      8\n      PAL0708\n      9\n      Gentoo penguin (Pygoscelis papua)\n      Anvers\n      Biscoe\n      Adult, 1 Egg Stage\n      N35A1\n      Yes\n      11/27/07\n      43.3\n      13.4\n      209.0\n      4400.0\n      FEMALE\n      8.13643\n      -25.32176\n      NaN\n    \n    \n      9\n      PAL0708\n      38\n      Adelie Penguin (Pygoscelis adeliae)\n      Anvers\n      Dream\n      Adult, 1 Egg Stage\n      N24A2\n      Yes\n      11/16/07\n      42.2\n      18.5\n      180.0\n      3550.0\n      FEMALE\n      8.04787\n      -25.49523\n      NaN\n    \n  \n\n\n\n\n\nfrom itertools import combinations\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\n\n#add all features\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Region\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    #cols = [\"Flipper Length (mm)\", \"Body Mass (g)\", \"Island_Dream\", \"Island_Torgersen\"]\n\n \n\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Body Mass (g)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Delta 15 N (o/oo)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Delta 13 C (o/oo)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Body Mass (g)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Delta 15 N (o/oo)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Depth (mm)', 'Delta 13 C (o/oo)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Flipper Length (mm)', 'Body Mass (g)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Flipper Length (mm)', 'Delta 15 N (o/oo)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Flipper Length (mm)', 'Delta 13 C (o/oo)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Body Mass (g)', 'Delta 15 N (o/oo)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Body Mass (g)', 'Delta 13 C (o/oo)']\n['Clutch Completion_No', 'Clutch Completion_Yes', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Body Mass (g)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Delta 15 N (o/oo)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Delta 13 C (o/oo)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Body Mass (g)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Delta 15 N (o/oo)']\n['Sex_FEMALE', 'Sex_MALE', 'Culmen Depth (mm)', 'Delta 13 C (o/oo)']\n['Sex_FEMALE', 'Sex_MALE', 'Flipper Length (mm)', 'Body Mass (g)']\n['Sex_FEMALE', 'Sex_MALE', 'Flipper Length (mm)', 'Delta 15 N (o/oo)']\n['Sex_FEMALE', 'Sex_MALE', 'Flipper Length (mm)', 'Delta 13 C (o/oo)']\n['Sex_FEMALE', 'Sex_MALE', 'Body Mass (g)', 'Delta 15 N (o/oo)']\n['Sex_FEMALE', 'Sex_MALE', 'Body Mass (g)', 'Delta 13 C (o/oo)']\n['Sex_FEMALE', 'Sex_MALE', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Flipper Length (mm)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Body Mass (g)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Delta 15 N (o/oo)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Delta 13 C (o/oo)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Body Mass (g)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Delta 15 N (o/oo)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Depth (mm)', 'Delta 13 C (o/oo)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Flipper Length (mm)', 'Body Mass (g)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Flipper Length (mm)', 'Delta 15 N (o/oo)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Flipper Length (mm)', 'Delta 13 C (o/oo)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Body Mass (g)', 'Delta 15 N (o/oo)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Body Mass (g)', 'Delta 13 C (o/oo)']\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n['Culmen Length (mm)', 'Culmen Depth (mm)']\n['Culmen Length (mm)', 'Flipper Length (mm)']\n['Culmen Length (mm)', 'Body Mass (g)']\n['Culmen Length (mm)', 'Delta 15 N (o/oo)']\n['Culmen Length (mm)', 'Delta 13 C (o/oo)']\n['Culmen Depth (mm)', 'Flipper Length (mm)']\n['Culmen Depth (mm)', 'Body Mass (g)']\n['Culmen Depth (mm)', 'Delta 15 N (o/oo)']\n['Culmen Depth (mm)', 'Delta 13 C (o/oo)']\n['Flipper Length (mm)', 'Body Mass (g)']\n['Flipper Length (mm)', 'Delta 15 N (o/oo)']\n['Flipper Length (mm)', 'Delta 13 C (o/oo)']\n['Body Mass (g)', 'Delta 15 N (o/oo)']\n['Body Mass (g)', 'Delta 13 C (o/oo)']\n['Delta 15 N (o/oo)', 'Delta 13 C (o/oo)']\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\n# this counts as 3 features because the two Clutch Completion \n# columns are transformations of a single original measurement. \n# you should find a way to automatically select some better columns\n# as suggested in the code block above\ncols = [\"Flipper Length (mm)\", \"Body Mass (g)\", \"Island_Dream\", \"Island_Torgersen\"]\n\nLR = LogisticRegression()\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n/Users/hedavamsolano/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\n0.78125\n\n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n0.6911764705882353\n\n\n\nfrom mlxtend.plotting import plot_decision_regions\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\ndef decision_region_panel(X, y, model, qual_features):  \n  p = len(qual_features)\n  fig, axarr = plt.subplots(1, p, figsize=(4*p,4))\n  for i in range(p):\n\n      filler_feature_values = {2+j: 0 for j in range(p)}\n\n      filler_feature_values.update({2+i: 1})\n\n      ix = X[qual_features[i]] == 1\n\n      ax = axarr[i]\n\n      plot_decision_regions(np.array(X[ix]), y[ix], clf=model,\n                            filler_feature_values=filler_feature_values,\n                            filler_feature_ranges={2+j: 0.1 for j in range(p)},\n                            legend=2, ax=ax)\n\n      ax.set_xlabel(X.columns[0])\n      ax.set_ylabel(X.columns[1])\n\n      handles, labels = ax.get_legend_handles_labels()\n      ax.legend(handles, \n          [\"Adelie\", \"Chinstrap\", \"Gentoo\"], \n           framealpha=0.3, scatterpoints=1)\n\n  # Adding axes annotations\n  fig.suptitle(f'Accuracy = {model.score(X, y).round(3)}')\n  plt.tight_layout()\n  plt.show()\n\n\nqual_features = [\"Island_Dream\", \"Island_Torgersen\"]\ndecision_region_panel(X_train[cols], y_train, LR, qual_features)\n\n/Users/hedavamsolano/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n  warnings.warn(\n/Users/hedavamsolano/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n  warnings.warn(\n\n\n\n\n\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\n\nclf = Pipeline([\n  ('feature_selection', SelectFromModel(LinearSVC(penalty='l1', dual=False))),\n  ('classification', RandomForestClassifier())\n])\nclf.fit(X_train, y_train)\nclf.score(X_train, y_train)\nclf[:-1].get_feature_names_out()\n\n/Users/hedavamsolano/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n\n\narray(['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)',\n       'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)',\n       'Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Sex_MALE'],\n      dtype=object)"
  },
  {
    "objectID": "posts/Perceptron-Blog-Post/index.html",
    "href": "posts/Perceptron-Blog-Post/index.html",
    "title": "Perceptron Blog",
    "section": "",
    "text": "Perceptron & Update Rule Overview\nGiven linearly separable data, the perceptron algorithm always converges as it correctly linearly separates the data in a finite number of steps. To do so, it produces a “good” vector of parameters (weights and bias) by first initializing the parameters (usually a random guess or 0), then looping through the given feature matrix and label array and updating these parameters to produce a better prediction on the current example by moving the parameters in the right direction if (and only if) the model’s prediction for said example is incorrect (does not match the example’s actual label).\nIn the fit() method, the perceptron update rule functions as follows:\n- 1*(((2*yi-1) * self.w@xi) <= 0) indicator function returns 1 if labels don't match and 0 if they do. \n- if labels don't match we add y (-1 or 1 representation of actual label) multiplied by xi (our sample vector) to our parameter vector in either the positive or negative direction, depending on our actual label's value; 0 or -1 (negative direction); 1 or +1 (positive direction)\nNOTE: In order to account for bias we modifiy our given feature matrix by adding column of 1’s and expand our weight vector to have an extra column (to store bias in).\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\nfrom perceptron import Perceptron\n\n\n\nLinearly Separable Example 1\n\nweight = 0\nhistory = [] \nmax_steps = 10000\n\np = Perceptron(weight,history)\n\nnp.random.seed(12345)\n\nn = 100 #the number of data points\np_features = 3 #the number of features ends up being 2 since we do n_features = p_features - 1\n\n\nX, y = make_blobs(n_samples = n, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n\n\n\np.fit(X,y, max_steps)\n\n\n\n\nprint(p.history[-10:])\nprint(p.w)\n\n\n\n\n[0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 0.99, 1.0]\n[2.10557404 3.1165449  0.25079936]\n\n\nIn this example the perceptron algorithm reaches a score of 1.0, meaning it was able to succesfully linearly separate the data.\n\nfig = plt.scatter(X[:,0], X[:,1], c = y) #means [ first_row:last_row , column_0]\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\n\nfig = draw_line(p.w, -4, 4)\n\n\n\n\nVisualization of the perceptron’s resulting linear separator\n\nfig = plt.plot(p.history) \nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\nVisualization of the number of iterations it took the perceptron algorithm to converge\n\n\nLinearly Separable Example 2\n\nweight2 = 0\nhistory2 = [] \nmax_steps = 1000000\n\np2 = Perceptron(weight,history)\n\nnp.random.seed(12345)\n\nn2 = 100 #the number of data points\np_features2 = 3 #the number of features ends up being 2 since we do n_features = p_features - 1\n\n\nX2, y2 = make_blobs(n_samples = n2, n_features = p_features2 - 1, centers = [(-1, -1.888), (10, 1.7)])\nX_2 = np.append(X2, np.ones((X2.shape[0], 1)), 1)\n\n\n\n\np2.fit(X2,y2, max_steps)\n\n\n\n#print(p2.score(X2,y2))\n\n\nprint(p2.history[-10:])\nprint(p2.w)\n\n\n[0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 1.0]\n[ 0.25738542  0.90742861 -1.74920064]\n\n\nIn this example the perceptron algorithm reaches a score of 1.0, meaning it was able to succesfully linearly separate the data.\n\nfig = plt.scatter(X2[:,0], X2[:,1], c = y2) \nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\n\nfig = draw_line(p2.w, -4, 12)\n\n\n\n\nVisualization of the perceptron’s resulting linear separator\n\nfig = plt.plot(p2.history) \nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\nVisualization of the number of iterations it took the perceptron algorithm to converge\n\n\nNon-linearly Separable Example\n\nweight3 = 0\nhistory3 = [] \nmax_steps = 10000\n\np3 = Perceptron(weight,history)\n\nnp.random.seed(12345)\n\nn3 = 100 #the number of data points\np_features3 = 4 #the number of features ends up being 2 since we do n_features = p_features - 1\n\n\nX3, y3 = make_blobs(n_samples = n3, n_features = p_features3 - 1, centers = [(-1, -1.7), (1.23, 1.7)])\nX_3 = np.append(X3, np.ones((X3.shape[0], 1)), 1)\n\n\np3.fit(X3,y3, max_steps)\n\n\nprint(p3.history[-10:])\nprint(p3.w)\n\n[0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98, 0.98]\n[ 1.436715    4.80069223 -0.74920064]\n\n\nThe score did not reach 1.0, so the perceptron algorithm wasn’t able to converge signifying that the data is not linearly separable.\n\nfig = plt.scatter(X3[:,0], X3[:,1], c = y2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color = \"black\")\n\n\nfig = draw_line(p3.w, -4, 4)\n\n\n\n\nVisualization of the perceptron’s resulting linear separator\n\nfig = plt.plot(p3.history) #plot is not representative of iterations (no such thing as .5 iterations) #gotta fix somehow w/ matplotlib adjusment probably\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nVisualization of the number of iterations it took the perceptron algorithm to converge\n\n\nMulti-Dimensional Example\n\nweight4 = 0\nhistory4 = [] \nmax_steps = 10000\np4 = Perceptron(weight,history)\n\nnp.random.seed(12345)\n\nn4 = 100 #the number of data points\np_features4 = 6 #the number of features ends up being 5 since we do n_features = p_features4 - 1 in make_blobs() call below\n\n\np_features10 = 3 #gives us 10 n_features\nX10, y10 = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-0.7, -1, 0.2, 0.1, 2 , 0.4, 0.3 , 2, 0.9, 1), (0.7, 1, -0.2, -0.1, -0.3, -0.4, -0.3 , -0.6, -0.9, -1)])\n\nprint(X10.shape) #to check that our data is multi-dimensional (5-dimensions in this case) as intended, meaning there are 5 features recorded for each sample.\n\np4.fit(X10,y10, max_steps)\n\n\nprint(p4.history[-10:])\nprint(p4.w)\n\n(200, 10)\n[0.995, 0.995, 0.995, 0.995, 0.995, 0.995, 0.995, 0.995, 0.995, 1.0]\n[ 3.01593611  4.9321606   1.38926737  2.26108177 -3.32174363 -2.79359171\n -2.39895371 -5.18644807 -1.71482216 -5.7152505   7.2800442 ]\n\n\nIn this example the perceptron algorithm reaches a score of 1.0, meaning it was able to succesfully linearly separate the data, even in more than 2 dimensions (5 in this experiment).\n\nfig = plt.plot(p4.history) #plot is not representative of iterations (no such thing as .5 iterations) #gotta fix somehow w/ matplotlib adjusment probably\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nVisualization of the number of iterations it took the perceptron algorithm to converge\n\n\nRuntime complexity of perceptron udpate rule\nThe runtime complexity of a single iteration of the perceptron update rule is O(n) where n represents our data’s number of features. To perfom the perceptron update rule we must compute the dot product of the parameter vector and one sample vector from our modified feature matrix both of which have an entry size that’s dependent on the data’s number of features, which has multiplication and addition as its relevant operations. Our data’s number of samples doesn’t affect the time complexity of a single iteration since we only end up working with one sample (chosen randomly) at the time of the update and picking and accessing a sample in the feature matrix takes constant time."
  },
  {
    "objectID": "posts/Linear Regression/index.html",
    "href": "posts/Linear Regression/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "The implementation of Linear Regression could be found here: https://github.com/Hedavam/Hedavam.github.io/blob/main/posts/Optimization%20for%20Logistic%20Regression/lr.py"
  },
  {
    "objectID": "posts/Linear Regression/index.html#least-squares-linear-regression",
    "href": "posts/Linear Regression/index.html#least-squares-linear-regression",
    "title": "Linear Regression",
    "section": "Least-Squares Linear Regression",
    "text": "Least-Squares Linear Regression\nSince we have a linear model, our predictions will be linear: \\(\\hat{y}_i = \\langle \\mathbf{w}, \\mathbf{x}_i \\rangle\\)\nThe loss function we will use for our implementation of linear regression is the squared loss: \\(\\ell(\\hat{y}, y) = (\\hat{y} - y)^2\\)\nTo obtain our desired parameter vector (weights and bias), we want to minimize the emprirical risk of our training model as shown below:\n\\[\\begin{aligned}\n\\hat{\\mathbf{w}} &= \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{w}} \\; L(\\mathbf{w}) \\\\\n          &= \\sum_{i = 1}^n \\ell(\\hat{y}_i, y_i) \\\\\n          &= \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{w}} \\sum_{i = 1}^n \\left(\\langle \\mathbf{w}, \\mathbf{x}_i \\rangle - y_i \\right)^2\\;.\n\\end{aligned}\\]\nThe equation takes the following form in matrix-vector notation:\n\\[\\begin{aligned}\n\\hat{\\mathbf{w}} = \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{w}} \\; L(\\mathbf{w}) = \\mathop{\\mathrm{arg\\,min}}_{\\mathbf{w}} \\; \\lVert \\mathbf{X}\\mathbf{w}- \\mathbf{y} \\rVert_2^2\\;.\n\\end{aligned}\\]\nSince squared loss is a convex function, we are guaranteed to find any local minimum, which will be the global minimum by the definition of convexity.\nBy the definition of convexity, this minimum is located at the point where the empirical risk function’s gradient is 0.\n\nAnalytic Approach\nIf we set the gradient to 0, we can solve our risk minimization formula explicitly for our parameter vector, as long as our feature matrix has as many rows (samples) as it has columns (features).\nStep-by-step process to this is outlined here: https://middlebury-csci-0451.github.io/CSCI-0451/lecture-notes/regression.html#solution-methods\n\n\nGradient Descent Approach\nOur gradient represents our descent direction. To do gradient descent, we iteratively calculate the gradient and adjust our parameter vector accordingly (param. vector - learning rate * grad). Thus, every iteration we move in descent direction until our algorithm converges (gradient = 0). The learning rate hyperparameter controls the speed of our descent.\nFor more info on gradient descent, reference this previous blog post: Optimization with Gradient Descent Blog Post\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\nfrom linearRegression import LinearRegression\n\n\n#This function helps modify our given feature array by adding column of 1's \ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\n#This function will create both testing and validation data \ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = 1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\n#choose # of samples and features\nn_train = 100\nn_val = 100\np_features = 1\n\n#adjusts variability of the data\nnoise = 0.05\n\n#create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n\n#create instance of our LinearRegression class\nweight = 0\nscore_history = []\nlr = LinearRegression(weight, score_history)\n\n\nlr.fit_gradient(X_train, y_train, alpha = 0.095, max_epochs = 2000)\nprint(\"with a parameter vector of \" + str(lr.w)) # inspect the fitted value of w\nprint(\"Algorithm converged after \" + str(len(lr.score_history)) + \" iterations\") \n\nlr.fit_analytic(X_train, y_train)\nprint(\"\\nAnalytic approach obtains a parameter vector of \" + str(lr.w))\n\nGradient approach conveged\nwith a parameter vector of [0.50246422 0.23658076]\nAlgorithm converged after 1232 iterations\n\nAnalytic approach obtains a parameter vector of [0.5024643  0.23658072]\n\n\nBoth linear regression fit methods produce a nearly identical parameter vector.\n\nprint(f\"Training score = {lr.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {lr.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.6637\nValidation score = 0.6797\n\n\n\n#Visualize the data and linear regression fits\n\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\n\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter((X_val), y_val)\n\n\n#pad for graphing \ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\naxarr[0].plot(pad(X_train)[:,0], pad(X_train)@lr.w, color = \"black\") #[:,0]gives 1st column vs. print(X_train[0]) #gives 1st enxtry\naxarr[1].plot(pad(X_val)[:,0], pad(X_val)@lr.w, color = \"black\") \n\n\nlabs = axarr[0].set(title = \"Training\", xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"Feature (x)\", ylabel = \"Target (y)\")\nplt.tight_layout()\n\n\n\n\nModel does a good job on both the training and validation data.\n\nplt.plot(lr.score_history)\nplt.ylim([0, 1])\nlabels = plt.gca().set(title = \"Score vs. Iterations\", xlabel = \"# of Iterations\", ylabel = \"Score\")\n\n\n\n\nOur chosen measure to score our model is the coefficient of determination, with 1 represnting perfect predictive accuracy. Score may be negative for bad models."
  },
  {
    "objectID": "posts/Linear Regression/index.html#lasso-regularization",
    "href": "posts/Linear Regression/index.html#lasso-regularization",
    "title": "Linear Regression",
    "section": "LASSO Regularization",
    "text": "LASSO Regularization\nLASSO adds a regularization term to our parameter vector, which shrinks the parameter vector’s weights associated with less important features (even to 0), which helps combat overfitting as our number of features increases. This may be used for feature selection and is useful when we have more features than samples (overparameterized problems).\n\n#choose # of samples and features\nn_train = 100\nn_val = 100\np_features = n_train - 1\n\n#adjusts variability of the data\nnoise = 0.5\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.001)\nL2 = Lasso(alpha = 0.00001)\nL3 = Lasso(alpha = 0.1)\n\nlasso_score_hist = []\nlasso_lil_score_hist = []\nlasso_big_score_hist = []\nval_score_hist = []\n\n#i is the p_features\nfor i in range(1, (n_train-1)+ 50):\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, i, noise) #Create Data; i is number of features\n    \n    L.fit(X_train, y_train)\n    lasso_score_hist.append((L.score(X_val, y_val)))\n    \n    L2.fit(X_train, y_train)\n    lasso_lil_score_hist.append((L2.score(X_val, y_val)))\n    \n    L3.fit(X_train, y_train)\n    lasso_big_score_hist.append((L3.score(X_val, y_val)))\n    \n    lr.fit_analytic(X_train, y_train) #fit\n    val_score_hist.append((lr.score(X_val, y_val))) #update model's validation score\n\n\n#Graph it\nfig, axarr = plt.subplots(1, 2, figsize=(15,5), sharex = False, sharey = False)\n\nnum_steps = len(lasso_score_hist)\n\naxarr[0].plot(np.arange(num_steps) + 1, lasso_score_hist, label = \"with lasso\")\naxarr[0].plot(np.arange(num_steps) + 1, val_score_hist, label = \"without lasso\")\n\n\n\naxarr[1].plot(np.arange(num_steps) + 1, lasso_score_hist, label = \"with lasso\")\naxarr[1].plot(np.arange(num_steps) + 1, val_score_hist, label = \"without lasso\")\n\n\naxarr[0].axis(ymin=0,ymax=1)\n\nlabs = axarr[0].set(title = \"Validation Score (0-1) vs. Features\", xlabel = \"# of Features\", ylabel = \"Validation Score\")\nlabs = axarr[1].set(title = \"Validation Score (unregulated) vs. Features\", xlabel = \"# of Features\", ylabel = \"Validation Score\")\n\nlegend = axarr[0].legend()\nlegend = axarr[1].legend()\n\n#ignore warnings for cleaner output\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n\n\nTake-away: LASSO Regularization has improved perfomance for overparametrized problems\nOur linear regression implementation matches up well with Sklearn’s LASSO up until the number of features gets close to the number of samples, at which point the validation score of our non-regularized model plummets while LASSO’s validation score stays above 0.\n\nnum_steps = len(lasso_score_hist)\n\nplt.plot(np.arange(num_steps) + 1, lasso_score_hist, label = \"alpha = 0.001\")\nplt.plot(np.arange(num_steps) + 1, lasso_lil_score_hist, label = \"alpha = 0.00001\")\nplt.plot(np.arange(num_steps) + 1, lasso_big_score_hist, label = \"alpha = 0.1\")\n\n\n\nplt.ylim([0,1])\nlabels = plt.gca().set(title = \"Scores vs. Features\", xlabel = \"# of Features\", ylabel = \"Score\")\nlegend = plt.legend()\n\n\n\n\n\n\nTake-away: Large learning rates (alphas) negatively affect Sklearn’s LASSO Regularization algorithm’s perfomance while small ones don’t have much of an effect on its perfomance (likely the algorithm is ran for more iterations to balance this out)"
  },
  {
    "objectID": "posts/Project Blog Post/index.html",
    "href": "posts/Project Blog Post/index.html",
    "title": "Project Blog Post",
    "section": "",
    "text": "In the world of soccer, the market value serves as one of the main sources for generating revenue for soccer clubs. Over the last few years, the money spent on the market value has sky-rocketed with FIFA, the governing body in soccer, revealing that soccer clubs spent $7.35 billion in player acquisitions just in 2019. In this high stake environment, with so much money involved, it is important for clubs to accurately assess the market value of players before submitting a bid to buy a player. However, the traditional crowd-sourced approaches have been questioned for their inconsistency and susceptibility to bias. Recognizing this challenge, we aim to build an alternative to the conventional crowd-sourced approach to determining a player’s market value, using the power of machine learning algorithms. Our project not only targets determining a player’s market value, but also addresses a more complex issue in soccer finance: the ‘transfer fee’. The ‘transfer fee’—the total cost a club pays for a player—extends beyond the player’s market value, incorporating additional monetary obligations imposed by selling clubs. In the project, we have developed a machine learning model that relies on linear regression and random forest algorithms to estimate the transfer fee. This model has the potential to help clubs and transfer agents for strategic financial planning and reducing the risk of overpayment."
  },
  {
    "objectID": "posts/Project Blog Post/index.html#data",
    "href": "posts/Project Blog Post/index.html#data",
    "title": "Project Blog Post",
    "section": "Data",
    "text": "Data\nOur project has been made possible through the compilation of data from three distinct websites, which served as primary sources for both training and testing our models.\nSofifa.com: This source primarily provides us with player ratings and potential scores and other attributes such as shooting scores, passing scores etc. from the popular video game series, FIFA, developed by EA Sports. The ratings are derived from the efforts of over 6,000 FIFA Data Reviewers, also known as Talent Scouts. (Murphy, 2021). This collaborative work helps ensuring that player ratings are continuously updated and reflective of their real-time performance .The site’s updates are carried out weekly, thereby providing us with relevant data.\nTransfermarkt.com: This platform is our primary source for club data, serving as a resource for statistics related to clubs that individual players play for. Transfermarkt is a popular entity in the soccer world, known for its estimation of player market values. However, these values have been subject to criticism in the past, as noted in this New York Times article (Smith,2021). Consequently, for the purpose of our project, we’ve chosen to focus solely on factual club data. This includes league positions over the years, player rosters, amount of money spent in the market value, goals scored, and other quantifiable statistics that are a reliable record of the club’s performance.\nKaggle.com: Originally, our strategy was to compile transfer data from 2018, coinciding with the final update to the data in FIFA 2017, just prior to the opening of the 2018 market value—a period of three months during which soccer clubs are permitted to trade players. However, given the limited number of player trades that occur each year and the resulting time-intensive nature of annual data scraping, we found a more efficient solution. We found a large dataset on Kaggle, containing transfer data from European leagues from 1992 to 2021. This data was published by a Kaggle user, Bhavik Chadna, and was initially scraped from TransferMarkt.com. In a significant enhancement to our methodology, we segmented this dataset by league and by year. This allowed us to merge data for each league, for each year, with corresponding player data from the respective FIFA years, as obtained from Sofifa.com. The final stage of our data processing involved aggregating all our segmented datasets into a singular, comprehensive CSV file. The resultant dataset comprises transfer data from 2015 to 2021, from prominent leagues including La Liga (Spain), Premier League (England), Ligue 1 (France), Serie A (Italy), and Bundesliga (Germany). This was subsequently aligned with player data from the corresponding years from Sofifa. In sum, our project utilizes data from Kaggle, effectively harmonizing it with data from Sofifa and Transfermarkt. This integrative approach ensures that our model is trained on the most comprehensive and accurate data possible.\nUpon gathering and filtering the data, we finalized with two distinct datasets for each of our project models.\nMarket Value Model: This model is centered around the dataset derived from Sofifa’s FIFA 2017 statistics. This dataset comprises a total of 5700 observations, each comprising 59 features. Looking back, we recognize that the volume of observations could have been significantly expanded by integrating data from multiple years of FIFA. However, due to time constraints and satisfactory performance of the model with the current dataset, we concluded that the expansion was not required.\nTransfer Fee Model: Our second model, focusing on transfer fees, was trained and tested on a dataset, with 619 observations and 78 distinct features. Although the dataset encompasses transfer records spanning from 2015 to 2021, the relatively small number of observations underlines an important aspect of the soccer market value: only a limited number of players are traded each year.\nOur sources hold a good ethical standard mostly because of the nature of sports data, including soccer, which is generally made public. Soccer clubs are usually open about player transfers, making our sources such as Transfermarkt essentially collection points for this readily available information."
  },
  {
    "objectID": "posts/Project Blog Post/index.html#approach",
    "href": "posts/Project Blog Post/index.html#approach",
    "title": "Project Blog Post",
    "section": "Approach",
    "text": "Approach\nNOTE: For much more on the specifics + useful resources that helped us w/ this portion of project, refer to: Project.ipynb file in Github Repository\nMarket Value Model:\nOur imported data was fairly clean and we had plenty of samples and features (5700, 59). When preparing our data for model training, we dropped categorical features like name and field position and any samples with features containing N/A values. Also, we dropped release clause and wage because these features seem to be calculated by Fifa with the same formula that is used for market value so they were biased predictors. After all this, we got down to 5010 samples and 57 features. We used 20% of this data as a holdout test set and randomized our test_train_split because our original data was ordered by the player’s overall rating and we didn’t want the model to train on “good players” and test on “bad players” at the tail end of our dataset. We standardized our data using Sklearn’s StandardScaler() so the coefficients of our linear predictors could be more easily interpreted. Our target vector was the “value” column of our data and the rest of the features comprised our feature matrix.\nThereafter, we performed feature selection. We combined univariate feature selection approaches like mutual information and ANOVA with recursive feature elimination (w/ cross-validation) using Lasso Linear Regression to select the “most important” predictive features.\nWe trained our models on the standout selected features from our univariate + RFE approach (Age and overall rating seemed to be the most important ones). We used a LASSO model and a RandomForestRegressor model to observe the difference between linear and non-linear patterns.\nTo examine model success, we computed the score (coefficient of determination) on the holdout set and used cross-validation as well to ensure that the ranndomness of the train test split wasn’t giving us optimistic or pessimistic results on the holdout set.\nWe also plotted learning curves, actual vs. predicted, and residuals to visually aid in examining our model’s underfitting/overfitting tendencies.\nTransfer Fee Model:\nOur imported data had 708 samples and 78 features but it was not very clean. We had to subset the data for modeling to just field players (excluded goalkeepers) because goalkeepers had N/A values for many of the non-goalkeeping related features. We got down to 414 samples and 73 features (dropped categorical ones) for our field player data that we would use for modeling. We used 20% of this as a houldout test set and randomized our test_train_split. We standardized our data using Sklearn’s StandardScaler() so the coefficients of our linear predictors could be more easily interpreted. Our target vector was the “fee_cleaned” column of our data (which we turned into millions) and the rest of the features comprised our feature matrix.\nBased on the literature from Ian G. McHale, Benjamin Holmes(https://www.sciencedirect.com/science/article/pii/S0377221722005082#bib0029), we engineered two features with high predictive power: avg. price paid by selling club and avg. price paid by buying club.\nWe performed the same process for feature selection as described for the market value model. The most notable and intuitive of these features were: ‘fee_cleaned_buyer_avg’(avg. price paid by buying club), ‘fee_cleaned_seller_avg’(avg. price paid by selling club), ‘value_eur’(market value). We also used age and potential as features because in the literature these features were deemed important by the author’s models.\nOnce again, we used a LASSO and Random Forest Regressor to obseve differences between the linear and ensemble approaches.\nGiven that this was a more complicated task than predicting market value, we tuned hyperparameters to squeeze out the best possible model performance for each model using nested cross-validation as we also wanted to use cross-validation to assess the success of the model with the best hyperparameters without optimistic biases.\nWe also computed the accuracy of the model on the holdout-set and used cross-validation, though neither of these measures were robust performance estimates as we had hoped them to be as is further explained in the results section.\nWe also plotted learning curves, actual vs. predicted, and residuals to visually aid in examining our model’s underfitting/overfitting tendencies.\nWe also performed an audit for bias by nationality. We picked 4 relatively well-represented countries (enough/similar amount of samples) in our dataset that are associated with great football players: England, France, Italy, Germany. First, we examined the disparities in “actual” transfer fee values for these countries in the dataset. We then used our “best” model for predictions to examine the disparities in “predicted” transfer fee values for these countries using both mean and median since outliers (though an integral and non-trivial aspect of the football market) made some predictions for some countries appear worse than others. We determined a threshold for “good” predictions and calculated the proportion of good predictions by country to further assess bias possibilities."
  },
  {
    "objectID": "posts/Project Blog Post/index.html#implementing-a-small-user-interface-for-our-model",
    "href": "posts/Project Blog Post/index.html#implementing-a-small-user-interface-for-our-model",
    "title": "Project Blog Post",
    "section": "Implementing a Small User Interface for our Model",
    "text": "Implementing a Small User Interface for our Model\nAfter receiving nearly 90% accuracy for our market value mode, we decided to build a small user interface for our model that, upon receiving a player’s name via user input, outputs the player’s predicted market value. We also integrated our market value model into the UI, which calculates how much a team would have to pay for a player.\nWe started by creating separate Python files for each of our market value codes and market value codes, which we then used as modules in our main Flask backend file. After we finished our modules, we used conditionals to incorporate them into our flask file, and each module would run upon the retrieval of each user’s input. A minor disadvantage of our program is that, because the model divides the data set into training and testing, not all players are available; thus, it may take a few input tries to obtain the market value of a specific player (for example, if the user enters “Robert Lewandowski” and he is in the testing data set, his value will be displayed; otherwise, the user may need to enter his name a couple of times until he is in the testing data set). We created our main front end designs using HTML, CSS, and JavaScript in one HTML file. We created our main front end designs in one HTML file by combining HTML, CSS, and JavaScript. The frontend itself was not complicated, as it only required sufficient time for page design, such as the colors, margins, and positions of each attribute visible in the UI.\nInstructions for using the UI: - Navigate to the Github repository. - Download all of the files because each one is important to the model and app. - Then open the Python files and navigate to the directory where you saved them on your computer. - Then, within the Python notebook, run app.py, then navigate to the command shell and run the following command: python app.py - You can experiment with the UI by clicking on the link provided in the command shell. Unfortunately, because this is a - built-in Flask webpage, the link cannot be shared; thus, this is the only way to experiment with the app. - When running the Market Value Model UI, player names are formatted like “R. Lewandowski,” where the first name is abbreviated to just the first letter with a period and the last name, whereas for some players with no big last name, such as “’Neymar Jr,” the first name is spelled out and the last name is kept as is. Whereas the player name is formatted with their full name for the Transfer Fee Model, such as “Robert Lewandowski”."
  },
  {
    "objectID": "posts/Project Blog Post/index.html#accuracy-measures",
    "href": "posts/Project Blog Post/index.html#accuracy-measures",
    "title": "Project Blog Post",
    "section": "Accuracy Measures:",
    "text": "Accuracy Measures:\n\nSummary:\nLASSO: About 75 % accuracy on both holdout & cross-validation approach\nRFR: About 95 % accuracy on both holdout & cross-validation approach\n\n\nLearning Curves From Project File:\n\n\n\nimage.png\n\n\n\n\nActual vs. Predicted + Residual Plot From Project File:\n\n\n\nimage.png\n\n\nTransfer Fee Model:\n\n\nSummary:\nNOTE: Scores fluctuate from about the 60-70 % range based on the train_test_split. This could be accounted for by refitting and rescoring through various train_test_split loops, but is very time consuming.\nLASSO: About 65 % accuracy using cross-validation; About 85 % on holdout set\nRFR: About 70 % accuracy using cross-validation approach; About\n\n\nLearning Curves From Project File:\n #### Take-away:\n\n\nActual vs. Predicted + Residual Plot From Project File:\n NOTE: The residual plots have different y_axis limits to account for data-specific outliers, so double checking the axis to interpret the residual plot instead of doing it spatially is important. #### Take-away: Both models are relatively accurate when predicting smaller transfer fees, but the higher the transfer fee goes, the less accurate the predictions are. Some outliers throw the models off."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/Unsupervised Learning with Linear Algebra/index.html",
    "href": "posts/Unsupervised Learning with Linear Algebra/index.html",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "",
    "text": "Experiment\nOverall goal: - Decompose an image of our choice using SVD. - Experiment with different # k of singular value components to be used in the decomposition - Develop compression factor parameter that if enabled will choose the # k of singular value components to be used in the decomposition.\nUseful Resource: https://medium.com/@thusharabandara/measure-the-compression-performance-of-an-image-compression-algorithm-ea68c1839ec6\n\nFetching Image + Turning it to Black & White\n\nimport PIL\nimport urllib\n\ndef read_image(url):\n    return np.array(PIL.Image.open(urllib.request.urlopen(url)))\n\n\nurl = \"https://tinypng.com/images/social/website.jpg\"\nimg = read_image(url)\n\n\nfig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\ngrey_img = to_greyscale(img)\n\n\naxarr[0].imshow(img)\naxarr[0].axis(\"off\")\naxarr[0].set(title = \"original\")\n\naxarr[1].imshow(grey_img, cmap = \"Greys\")\naxarr[1].axis(\"off\")\naxarr[1].set(title = \"greyscale\")\n\n[Text(0.5, 1.0, 'greyscale')]\n\n\n\n\n\n\n\nDecomposition + Analysis\nIn the following function, we will pass our black and white image, decompose it and examine results.\n\ndef svd_reconstruct(image, k, cf=None):\n    \n    A = image #our image is m*n matrix\n    U, sigma, V = np.linalg.svd(A)#decompose with built in numpy svd\n    \n    m,n = image.shape[0], image.shape[1]\n    og_img_px = m*n\n    print(f\"The # of pixels needed to store original image is {og_img_px}\")\n    \n    # create the D matrix in the SVD\n    D = np.zeros_like(A,dtype=float) # matrix of zeros of same shape as A\n    D[:min(A.shape),:min(A.shape)] = np.diag(sigma) # singular values on the main diagonal\n\n    #Calculating Compression Factor\n    #compression factor is orinal/compressed \n    #original = A; compressed = A_\n    #A = m*n\n    #A_ = k*m + k(diagonals matrix is 0'd out so use k instead of k*k) + n*k\n    #cf = A/A_\n    #cf = m*n/k*m + k + n*k\n    #cf = m*n/k(m+1+n)\n    #cf(m+1+n) = m*n/k\n    #k*cf(m+1+n)= m*n\n    #k = m*n/cf(m+1+n)\n    \n    choose_k = 0\n    if(cf != None):\n        choose_k = int(((m*n) / (cf * (m+1+n))))\n    \n    if (choose_k > 0):\n        \n        U_ = U[:,:choose_k]\n        D_ = D[:choose_k, :choose_k]\n        V_ = V[:choose_k, :]\n    \n        A_ = U_ @ D_ @ V_\n        new_img_px = choose_k*m + choose_k + n*choose_k\n        print(f\"The # of pixels needed to store compressed image is {new_img_px}\\n\")\n        \n        return A_\n    \n    U_ = U[:,:k]\n    D_ = D[:k, :k]\n    V_ = V[:k, :]\n    \n    A_ = U_ @ D_ @ V_\n    \n    new_img_px = k*m + k + n*k\n    print(f\"The # of pixels needed to store compressed image is {new_img_px}\\n\")\n     \n    return A_\n\n\n#compare original size to new size\nsvd_reconstruct(grey_img,25)\nsvd_reconstruct(grey_img,8)\n\nsvd_reconstruct(grey_img,8, cf = 70)\nsvd_reconstruct(grey_img,8, cf = 5)\n\nThe # of pixels needed to store original image is 520200\nThe # of pixels needed to store compressed image is 38275\n\nThe # of pixels needed to store original image is 520200\nThe # of pixels needed to store compressed image is 12248\n\nThe # of pixels needed to store original image is 520200\nThe # of pixels needed to store compressed image is 6124\n\nThe # of pixels needed to store original image is 520200\nThe # of pixels needed to store compressed image is 102577\n\n\n\narray([[-166.58691639, -166.83674027, -171.06369029, ...,  -74.47347323,\n        -102.7494874 , -129.49640076],\n       [-171.05893064, -171.90815828, -177.41906167, ...,  -56.2651186 ,\n         -92.05222797, -121.56358122],\n       [-178.59615127, -178.46205866, -183.14545662, ...,  -51.42475947,\n         -90.07937304, -121.04338246],\n       ...,\n       [ -89.64106677,  -82.82397266,  -87.57796423, ..., -105.63327921,\n        -130.67409141, -140.58034411],\n       [ -81.86134331,  -74.64474276,  -79.75364623, ..., -105.92576626,\n        -123.65154946, -127.40840671],\n       [ -80.18976813,  -72.07005171,  -77.28038033, ..., -101.24308323,\n        -118.08228953, -119.9544055 ]])\n\n\n\nTake-Aways:\n\nThe higher the k, the less compression is done.\nThe higher the compression factor, the smaller the k, so more compression is done.\n\n\n\n\nEffect of Decomposition\nIn the following function, we will use our svd_reconstruct() function to perform an experiment in which we iteratively increase the # k of singular value components to be used in the decomposition and see how it affects image quality and storage.\n\ndef svd_experiment(image):\n    \n    #arrays to store compressed images, scores\n    img_array = []\n    score_calc = []\n\n    #Calculating Compression Rate\n    #compression rate is compressed/original \n    m,n = image.shape[0], image.shape[1]\n    original_image_pixels = m * n \n    \n    for i in range(5,56,10): #i is the k\n        img_array.append(svd_reconstruct(image, k = i))\n        \n        #compression ratio\n        reduced_image_pixels = (m * i) + i + (i*n)\n        score_calc.append((reduced_image_pixels / original_image_pixels*100))\n    \n    fig, axarr = plt.subplots(2, 3, figsize=(15,5), sharex = False, sharey = False)\n    \n    #indices\n    img_index = 0\n    component_index = 5\n    \n    #loop through subplots\n    axarr = axarr.flatten()\n    for a in axarr:\n        a.imshow(img_array[img_index], interpolation='none', cmap = \"Greys\")\n        a.axis(\"off\")\n        a.set(title = str(component_index) + \" components, \" + \"storage: \" + str(round((score_calc[img_index]), 4)) + \"%\")\n        component_index = component_index + 10\n        img_index = img_index + 1\n\n\nsvd_experiment(grey_img)\n\nThe # of pixels needed to store original image is 520200\nThe # of pixels needed to store compressed image is 7655\n\nThe # of pixels needed to store original image is 520200\nThe # of pixels needed to store compressed image is 22965\n\nThe # of pixels needed to store original image is 520200\nThe # of pixels needed to store compressed image is 38275\n\nThe # of pixels needed to store original image is 520200\nThe # of pixels needed to store compressed image is 53585\n\nThe # of pixels needed to store original image is 520200\nThe # of pixels needed to store compressed image is 68895\n\nThe # of pixels needed to store original image is 520200\nThe # of pixels needed to store compressed image is 84205\n\n\n\n\n\n\n\nTake-Away: With only about 16% of the pixels available from original matrix being used to display the image, it is almost undistinguishable from the original image, which shows the power of dimensionality reduction."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My CSCI 0451 Blog",
    "section": "",
    "text": "A blog post of an implementation of Optimization for Logistic Regression with experiments\n\n\n\n\n\n\nOct 6, 2023\n\n\nHedavam Solano\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post where we try to predict market values and trasnfer fees of soccer players.\n\n\n\n\n\n\nMay 23, 2023\n\n\nHedavam Solano\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post where we do image compression with the singular value decomposition\n\n\n\n\n\n\nMay 23, 2023\n\n\nHedavam Solano\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post of an implementation of Linear Regression with experiments on regularization and a real data set.\n\n\n\n\n\n\nApr 21, 2023\n\n\nHedavam Solano\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nExamining sex-based algorithmic bias using an income classifiction model.\n\n\n\n\n\n\nApr 19, 2023\n\n\nHedavam Solano\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nPrepare for and reflect on virtual talk from Dr. Gebru on bias and social impact of AI.\n\n\n\n\n\n\nApr 18, 2023\n\n\nHedavam Solano\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post of an implementation of Perceptron with experiments\n\n\n\n\n\n\nMar 6, 2023\n\n\nHedavam Solano\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]