{
 "cells": [
  {
   "cell_type": "raw",
   "id": "82b98597-17e8-4a1e-a405-7a6a71a2e987",
   "metadata": {},
   "source": [
    "---\n",
    "title: Learning from Timnit Gebru\n",
    "author: Hedavam Solano\n",
    "date: '2023-04-18'\n",
    "image: \"image.jpg\"\n",
    "description: \"Prepare for and reflect on virtual talk from Dr. Gebru on bias and social impact of AI.\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298b95d4-4e62-4428-9ae7-3be90ffab9af",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bcf9c4-cce8-4e6c-b514-6a02b71a8df6",
   "metadata": {},
   "source": [
    "## About Dr. Gebru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d7bcff-8a5d-4460-ae14-c8c6683fbc3e",
   "metadata": {},
   "source": [
    "Dr. Gebru is an Ethiopian American computer scientist who specializes in algorithmic bias. At 15 years of age, she fled the Eritreanâ€“Ethiopian War and lived in Ireland for a bit until her request for a U.S Visa got approved after it was denied the first time. In her new school environments, Gebru faced discrimination on the basis of being an African refugee as some of her teachers didn't allow her to take higher level classes despite her prolific academic sucess. Furthemore, being inspired by a racist run-in with police, in which her friend had been assualted in a bar and then wrongfully arrested despite being the victim, Gebru decided to specialize int the field of ethics in technology. Her departure from Google epitimizes her work's essence as she states on her LinkedIn that she was fired from Google \"for raising issues of discrimination in the workplace.\" after she disputed Google's request to withdraw a research paper dealing with sensitive topics. Her impressive career in the tech industry that has been widely recognized as she was one of the 100 most influential people by *Time* magazine amongst many other honorary distinctions.\n",
    "\n",
    "Dr. Gebru will be giving a virtual talk at Middlebury on the bias and social impacts of AI on April 24, 2023 at 7:00 PM ET in the Franklin Environmental Center at Hillcrest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c54de02-0117-4f89-9ada-94eb3d676ca2",
   "metadata": {},
   "source": [
    "## Key Points from Dr. Gebru's talk as part of a Tutorial on Fairness, Accountability, Transparency, and Ethics (FATE) in Computer Vision "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca1af93-5cff-4384-9449-790914470a04",
   "metadata": {},
   "source": [
    "### Plausible Negative implications of AI\n",
    "\n",
    "With a single point of failure (glitch), if a hiring algorithm becomes widely used, certain groups of people might fail to get jobs. The face recognition algorithm identifying protesters through their social media was used by police to target protesters for unrelated things. From these examples we can conclude that faulty AI or AI in the wrong hands can impact our society negatively.\n",
    "\n",
    "On a more micro level, there are questions about the validity of the data that informs AI as it often reflects patterns of inclusion & exclusion. Diversity in datasets is not the norm. \n",
    "Tech companies have recognized this issue and tried to alleviate it, but visibility is not inclusion. Dr. Gebru points out many examples of tech companies using predatory methods like pulling transgender people's videos without consent for the purposes of diversifying their datasets.\n",
    "\n",
    "Dr. Gebru points out that an algorithm's measure of fairness is not necessarily it's perfomance being equal for different groups since there's other factors like error rate balance that may prove an algorithm to be unfair.\n",
    "\n",
    "Surely, representation is not an AI-specific issue, but AI's potential power may amplify the negative effects of misrepresentation.\n",
    "\n",
    "\n",
    "\n",
    "Dr. Gebru concludes by urging us to ask questions like:\n",
    "- Who is funding/endorsing & developing AI models?\n",
    "    - Usually, the dominant group\n",
    "- Who is negatively affected by unfairness in these models?\n",
    "    - Usually, the marginalized groups\n",
    "    \n",
    "Looking into the future, Dr. Gebru points out that by incorporating more people from marginalized communities in the world of AI (which is currently very homogenous), the algorithmic bias of models will be diminished.\n",
    "\n",
    "\n",
    "\n",
    "tl;dr: As a society, we need to be more aware of AI's inherent biases and how it may uphold the status quo, and we should diversify the AI field to progressively eliminate algorithmic biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc669d2-02b6-450c-88fc-3bbb791d6326",
   "metadata": {},
   "source": [
    "## Questions Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4b3de4-a08c-459b-8c25-ec528b1e0eb3",
   "metadata": {},
   "source": [
    "### Main Proposed Question:\n",
    "Given your experience as the co-leader of an ethics of AI team at Google, do you think algorithmic bias auditing should be overseen by a separate department within tech companies or would it be better for trusted independent companies to do bias auditing in order to keep big tech companies in check?\n",
    "\n",
    "### Other Questions:\n",
    "On DAIR's website, there are 2 linked research projects (The Legacy of Spatial Apartheid and Testing and Documentation).\n",
    "Is DAIR working on some new projects or updating these current projects?\n",
    "\n",
    "What are some interesting findings that have been made with the visual dataset of South Africa DAIR helped create?\n",
    "\n",
    "What were some difficulties making the datasheet for the visual dataset of South Africa DAIR helped create?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390a2727-690e-4d5b-8960-5ba252894bff",
   "metadata": {},
   "source": [
    "# Part 2: Notes on the Talk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d674793-e62d-41b0-9fe6-db4120ff8793",
   "metadata": {},
   "source": [
    "Dr. Gebru began her argument by giving a brief introduction of AGI and how some people believe it will be used to achieve an utopian society. \n",
    "\n",
    "In the first section of the talk, Dr. Gebru focused on discussing second-wave eugenics and more specifically the properties of the TESCREAL bundle (transhumanism, extropianism, singularitarianism, cosmism, Rationalism, effective altruism, and longtermism). She went through each term, providing definitions for the specific ideology, naming people associated with it, and laid out the ties between AGI and these ideologies. A memorable ideology of the TESCREAL was cosmism. Dr. Gebru pointed out how Goertzel, one of the researchers that \"christened\" the term AGI believe that \"morally superior beings produced by AGI may advance the good of the cosmos\" (@timnitGebru on Twitter). I think what Dr. Gebru wanted us to take-away from this section of the talk is that the ideologies of the TESCREAL bundle are rooted in eugenics and are a negative thing to advance by developing AGI. \n",
    "\n",
    "The next section of the talk focused on the two extreme eschatalogical implications of AGI: utopia and apocalypse. An AGI utopia as envisioned by some consists of AGI being able to find out the \"best\" thing to do in any scenario and ideally helping us achieve world peace. Others think an AGI utopia will be achieved through \"morally superior\" AGI enhanced transhuman minds that will benefit our society. Dr. Gebru urged us to question who an AGI utopia would be benefitting - claiming that it wouldn't benefit those in marginalized communities, but would instead benefit those in dominant positions of power. Regarding the AGI apocalypse, Dr. Gebru emphasized the importance of acknowdledging the systems driving the \"machines\" down dangerous paths and the negative impacts of their decisions. Namely, big corporations and the military funding research in these areas, which result in the environment being harmed, datasets being stolen, and workers being exploited.\n",
    "\n",
    "Her preferred alternative to AGI seemed to be having subject area experts curating datasets and working on smaller models with a specific use case as she pointed out in her example of the language translation some of her colleagues worked on. \n",
    "\n",
    "My perspective:\n",
    "I'm marveled by AGI tools like ChatGPT and think that they are more useful than they are harmful, for right now. However, I share the same worries many people do when it comes to the advancement in this technology as I do not believe that equity will be at its forefront. Like we've learned in ML, there's always a trade-off. These tools are and will likely remain for-profit, meaning there has to be a loss somewhere. I fear that those who lose will be those who have been losing throughout the course of history because of oppressive systems. Sure, AGI doesn't require much labor as the goal is for it to learn on its own, but to develop AGI powerful enough to stand on its own, many people will disadvantaged. Also, it's scary to think about the job market if AGI is succesfully develop. It's hard to imagine a world where most jobs are done by machines. Production might be increased, but perhaps at the expense of quality. If AGI becomes mainstream, will the majority of jobs available be in the CS field? A whole society of computer scientists doesn't sound like the best thing ever. Creativity surely would decline. I'm getting way ahead of myself as I don't think (or at least hope) we're anywhere close to experiencing these huge shifts caused by AGI in our society, but these are just the thoughts that run through my mind when thinking of how far left these things can go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18d7a0f-55cb-4b13-81f7-8c6476eb28de",
   "metadata": {},
   "source": [
    "# Part 3: Reflection on the Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add8df76-e01e-46b7-84c5-8a5269d45c3c",
   "metadata": {},
   "source": [
    "I appreciated that Dr. Gebru asks us to look at AI with a critical eye. I was unaware of how unethical data collection and content moderation practices could be and will be more keen to things like that in the future. With these ethical concerns in mind, I hope to be able to contribute to building more ethical AI. I felt empowered by Dr. Gebru's take on impostor syndrome as she said been in an environment conducive to shared progress helps you feel less like an impostor. She also reminded us that there are many systems in place that make you feel like an impostor, when, in fact, you are not.\n",
    "\n",
    "Something about the process I found discouraging was the extreme beliefs held by people with influence in the AI field. I doubt (and pray) that there will not be an AGI apocalypse of AGI utopia, but many people feel very strongly about either being the case. The animosity that ensued from the first question posed after the talk made me feel uncomfortable and I think is cause for concern as the rise of AGI will surely propogate more moments like that (and perhaps some with more serious consequences for others)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa9dacb-c55e-4104-adc3-72f09921f9e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml-0451] *",
   "language": "python",
   "name": "conda-env-ml-0451-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
