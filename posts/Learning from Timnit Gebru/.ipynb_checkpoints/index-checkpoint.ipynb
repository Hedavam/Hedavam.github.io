{
 "cells": [
  {
   "cell_type": "raw",
   "id": "82b98597-17e8-4a1e-a405-7a6a71a2e987",
   "metadata": {},
   "source": [
    "---\n",
    "title: Learning from Timnit Gebru\n",
    "author: Hedavam Solano\n",
    "date: '2023-04-18'\n",
    "image: \"image.jpg\"\n",
    "description: \"Prepare for and reflect on virtual talk from Dr. Gebru on bias and social impact of AI.\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298b95d4-4e62-4428-9ae7-3be90ffab9af",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bcf9c4-cce8-4e6c-b514-6a02b71a8df6",
   "metadata": {},
   "source": [
    "## About Dr. Gebru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d7bcff-8a5d-4460-ae14-c8c6683fbc3e",
   "metadata": {},
   "source": [
    "Dr. Gebru is an Ethiopian American computer scientist who specializes in algorithmic bias. At 15 years of age, she fled the Eritreanâ€“Ethiopian War and lived in Ireland for a bit until her request for a U.S Visa got approved after it was denied the first time. In her new school environments, Gebru faced discrimination on the basis of being an African refugee as some of her teachers didn't allow her to take higher level classes despite her prolific academic sucess. Furthemore, being inspired by a racist run-in with police, in which her friend had been assualted in a bar and then wrongfully arrested despite being the victim, Gebru decided to specialize int the field of ethics in technology. Her departure from Google epitimizes her work's essence as she states on her LinkedIn that she was fired from Google \"for raising issues of discrimination in the workplace.\" after she disputed Google's request to withdraw a research paper dealing with sensitive topics. Her impressive career in the tech industry that has been widely recognized as she was one of the 100 most influential people by *Time* magazine amongst many other honorary distinctions.\n",
    "\n",
    "Dr. Gebru will be giving a virtual talk at Middlebury on the bias and social impacts of AI on April 24, 2023 at 7:00 PM ET in the Franklin Environmental Center at Hillcrest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c54de02-0117-4f89-9ada-94eb3d676ca2",
   "metadata": {},
   "source": [
    "## Key Points from Dr. Gebru's talk as part of a Tutorial on Fairness, Accountability, Transparency, and Ethics (FATE) in Computer Vision "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca1af93-5cff-4384-9449-790914470a04",
   "metadata": {},
   "source": [
    "### Plausible Negative implications of AI\n",
    "\n",
    "With a single point of failure (glitch), if a hiring algorithm becomes widely used, certain groups of people might fail to get jobs. The face recognition algorithm identifying protesters through their social media was used by police to target protesters for unrelated things. From these examples we can conclude that faulty AI or AI in the wrong hands can impact our society negatively.\n",
    "\n",
    "On a more micro level, there are questions about the validity of the data that informs AI as it often reflects patterns of inclusion & exclusion. Diversity in datasets is not the norm. \n",
    "Tech companies have recognized this issue and tried to alleviate it, but visibility is not inclusion. Dr. Gebru points out many examples of tech companies using predatory methods like pulling transgender people's videos without consent for the purposes of diversifying their datasets.\n",
    "\n",
    "Dr. Gebru points out that an algorithm's measure of fairness is not necessarily it's perfomance being equal for different groups since there's other factors like error rate balance that may prove an algorithm to be unfair.\n",
    "\n",
    "Surely, representation is not an AI-specific issue, but AI's potential power may amplify the negative effects of misrepresentation.\n",
    "\n",
    "\n",
    "\n",
    "Dr. Gebru concludes by urging us to ask questions like:\n",
    "- Who is funding/endorsing & developing AI models?\n",
    "    - Usually, the dominant group\n",
    "- Who is negatively affected by unfairness in these models?\n",
    "    - Usually, the marginalized groups\n",
    "    \n",
    "Looking into the future, Dr. Gebru points out that by incorporating more people from marginalized communities in the world of AI (which is currently very homogenous), the algorithmic bias of models will be diminished.\n",
    "\n",
    "\n",
    "\n",
    "tl;dr: As a society, we need to be more aware of AI's inherent biases and how it may uphold the status quo, and we should diversify the AI field to progressively eliminate algorithmic biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc669d2-02b6-450c-88fc-3bbb791d6326",
   "metadata": {},
   "source": [
    "## Questions Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4b3de4-a08c-459b-8c25-ec528b1e0eb3",
   "metadata": {},
   "source": [
    "### Main Proposed Question:\n",
    "Given your experience as the co-leader of an ethics of AI team at Google, do you think algorithmic bias auditing should be overseen by a separate department within tech companies or would it be better for trusted independent companies to do bias auditing in order to keep big tech companies in check?\n",
    "\n",
    "### Other Questions:\n",
    "On DAIR's website, there are 2 linked research projects (The Legacy of Spatial Apartheid and Testing and Documentation).\n",
    "Is DAIR working on some new projects or updating these current projects?\n",
    "\n",
    "What are some interesting findings that have been made with the visual dataset of South Africa DAIR helped create?\n",
    "\n",
    "What were some difficulties making the datasheet for the visual dataset of South Africa DAIR helped create?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml-0451] *",
   "language": "python",
   "name": "conda-env-ml-0451-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
